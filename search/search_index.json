{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Utkarsh AI Labs","text":"<p>A 6-month AI/ML learning academy. Theory, code, proof-of-work commits.</p> Month Focus 1 Mathematical Foundations (Linear Algebra, Probability, Statistics, Optimization, Python for ML, SQL Basics) 2 Classical ML &amp; Feature Engineering (Regression, Trees, SVM, Ensembles, Feature Selection, Model Evaluation, EDA, Advanced SQL) 3 Deep Learning &amp; NLP (Neural Networks, CNNs, RNNs, Transformers, BERT Fine-tuning, HuggingFace, Model Evaluation) 4 MLOps &amp; Deployment (FastAPI, Docker, MLflow, CI/CD, AWS/GCP Basics, Model Monitoring, Kubernetes Fundamentals) 5 GenAI &amp; LLM Engineering (RAG Pipelines, Vector Databases, LangChain/LangGraph, LoRA Fine-tuning, Multi-Agent Systems, LLM Evaluation) 6 Capstone &amp; Portfolio (Production AI System, End-to-End Deployment, Monitoring, Technical Blogging, GitHub Optimization, Job Applications &amp; Mock Interviews) <p>Current: Month 1 \u2014 Foundations \u00b7 Dashboard</p>"},{"location":"dashboard/","title":"DASHBOARD","text":""},{"location":"certifications/","title":"Certifications","text":""},{"location":"daily-schedule/","title":"Daily Schedule","text":"<p>This schedule is designed for elite-level output, mimicking the rigor of a residential IIT program. By treating your learning like a professional \"residency,\" you're creating a high-trust signal for recruiters.</p>"},{"location":"daily-schedule/#the-ai-residency-daily-schedule","title":"\ud83d\udcc5 The \"AI Residency\" Daily Schedule","text":"<p>Goal: 6-8 hours of deep, focused intellectual work per day.</p> Time Activity Hardcore Phase Equivalent High-Performance Notes 06:00 \u2013 06:30 Morning Review Squad Sync / Review Active Recall: Write 5 key ideas from yesterday without looking at notes. 06:30 \u2013 08:00 DSA / LeetCode Daily Comp. Practice Neetcode/Striver. Strict 45 min per problem, then review the optimal solution. 08:00 \u2013 08:30 Breakfast + Break \u2014 Fuel up. 08:30 \u2013 10:30 Deep Work Block 1 Concepts Drilldown Focus: Video/Reading + Anki. Phone in another room. 10:45 \u2013 13:15 Deep Work Block 2 Hands-On Application The Rule: No tutorials. Build from a blank file. Break things to learn. 13:15 \u2013 14:00 Lunch + Decompress \u2014 Rest your brain. 14:00 \u2013 16:00 Deep Work Block 3 Project Build Work on weekly project deliverables. Measurable GitHub commits only. 16:00 \u2013 17:00 Advanced Topics Build Harder 1 arXiv abstract per day or SQL practice. 17:00 \u2013 18:00 Exercise Sports Time Non-Negotiable: Run, gym, or walk. Physicality drives mental clarity. 18:00 \u2013 19:00 Wrap &amp; Plan Daily Wrap Review what you built, plan tomorrow\u2019s 3 \"Big Wins,\" 15 min Anki. 19:00 \u2013 20:00 Community / Proof Social/Industry Sessions Post progress on LinkedIn/X. Compound your assets. 20:00 \u2013 21:00 Mock Prep / Certs Comp. Coding Hour Rotate daily: ML theory, behavioral prep, or Cert study. 21:00+ Wind Down Recovery Sleep by 22:30: Elite performance requires elite recovery."},{"location":"daily-schedule/#the-weekly-rhythm","title":"\ud83c\udf0a The Weekly Rhythm","text":"<p>This cycle ensures you don't just \"learn\" but \"produce\" at an industry-standard level.</p>"},{"location":"daily-schedule/#monday-the-blueprint","title":"Monday: The Blueprint","text":"<ul> <li>Task: Start new module.</li> <li>Deliverable: Set 3 Non-Negotiable Wins for the week (e.g., \"Implement Transformer Layer\").</li> </ul>"},{"location":"daily-schedule/#tuesday-the-coding-forge","title":"Tuesday: The Coding Forge","text":"<ul> <li>Task: Pure project development.</li> <li>Deliverable: Minimum 4 hours of raw code written and pushed to GitHub.</li> </ul>"},{"location":"daily-schedule/#wednesday-the-pressure-cooker","title":"Wednesday: The Pressure Cooker","text":"<ul> <li>Task: Technical interviewing/System Design.</li> <li>Deliverable: 1 session on Pramp or a mock-interview with a peer.</li> </ul>"},{"location":"daily-schedule/#thursday-the-research-lab","title":"Thursday: The Research Lab","text":"<ul> <li>Task: arXiv deep dive.</li> <li>Deliverable: Read 1 full paper; write a 3-bullet summary for your website's \"Research\" section.</li> </ul>"},{"location":"daily-schedule/#friday-the-documentation-polish","title":"Friday: The Documentation Polish","text":"<ul> <li>Task: Code review and GitHub cleanup.</li> <li>Deliverable: Refine your <code>README.md</code> files and ensure your code is commented/professional.</li> </ul>"},{"location":"daily-schedule/#saturday-the-8-hour-sprint","title":"Saturday: The 8-Hour Sprint","text":"<ul> <li>Task: Simulate a \"Hackathon\" environment.</li> <li>Deliverable: Finalize the week's project deliverable. Total completion.</li> </ul>"},{"location":"daily-schedule/#sunday-the-audit-rest","title":"Sunday: The Audit &amp; Rest","text":"<ul> <li>Task: KPI Review (Reviewing progress vs. roadmap).</li> <li>Deliverable: Plan next week\u2019s modules, rest, and organize personal tasks.</li> </ul>"},{"location":"daily-schedule/#why-this-works","title":"\ud83d\udee1\ufe0f Why This Works","text":"<ol> <li>Iterative Depth: You aren't just watching videos; you are recalling (Morning), applying (Deep Work 2), and explaining (LinkedIn/Academy).</li> <li>Professional Visibility: By 19:00 daily, you are building a \"Digital Paper Trail\" that recruiters find through Google search.</li> <li>Physical Integrity: The 17:00 exercise slot prevents the burnout that kills most 6-month learning journeys.</li> </ol>"},{"location":"dsa-sql-practice/","title":"DSA &amp; SQL Practice","text":""},{"location":"month-01/","title":"Month 1 \u2014 Foundations","text":""},{"location":"month-01/#month-1-foundations_1","title":"Month 1 \u2014 Foundations","text":""},{"location":"month-01/week-01-python-git-linux/git/","title":"Git","text":""},{"location":"month-01/week-01-python-git-linux/linux/","title":"Linux","text":""},{"location":"month-01/week-01-python-git-linux/python/","title":"Python","text":""},{"location":"month-01/week-01-python-git-linux/python/#aiml-python-stack-mastery-index-0-elite","title":"AI/ML Python Stack Mastery Index (0 \u2192 Elite)","text":""},{"location":"month-01/week-01-python-git-linux/python/#1-core-python-programming","title":"1. Core Python Programming","text":"Topic Area Beginner (Foundations) Intermediate (Engineering) Advanced / Elite (Optimization &amp; Internals) Syntax &amp; Data Variables, Primitive Types, Type Conversion Type Hinting (<code>typing</code>), Data Classes (<code>dataclasses</code>) Metaclasses, Descriptors, <code>__slots__</code> for memory Control Flow If/Else, For/While Loops List/Dict Comprehensions, <code>zip</code>, <code>enumerate</code> Generators (<code>yield</code>), Coroutines (<code>async/await</code>) Functions Definition, Arguments, Return values <code>*args</code>, <code>**kwargs</code>, Lambda, Map/Filter Decorators (with args), Closures, Functools Object Oriented Classes, Objects, Basic Methods Inheritance, Polymorphism, Magic Methods (<code>__init__</code>) Abstract Base Classes, Metaprogramming, Mixins Modules &amp; Env Import, Pip install, Basic Scripts Virtual Environments (<code>venv</code>, <code>poetry</code>), Package structure Relative imports, Namespace packages, Wheel building File I/O Read/Write TXT, CSV, JSON Context Managers (<code>with</code>), Pathlib Memory Mapping (<code>mmap</code>), Binary protocols, Pickle safety Error Handling Try/Except, Basic Errors Custom Exceptions, Logging module Context propagation, Debugging profilers (<code>cProfile</code>) Concurrency Single-threaded scripts <code>threading</code> (I/O), <code>subprocess</code> <code>multiprocessing</code> (CPU), GIL bypass, AsyncIO loops Performance Writing working code Vectorization awareness, Algorithmic complexity C-Extensions (<code>PyBind11</code>), JIT (<code>Numba</code>), Memory profiling"},{"location":"month-01/week-01-python-git-linux/python/#2-numpy-numerical-computing","title":"2. NumPy (Numerical Computing)","text":"Topic Area Beginner (Foundations) Intermediate (Engineering) Advanced / Elite (Optimization &amp; Internals) Arrays <code>np.array</code>, <code>np.zeros</code>, <code>np.ones</code> <code>np.arange</code>, <code>np.linspace</code>, Data types (<code>dtype</code>) Structured arrays, Record arrays, Memory views Manipulation <code>reshape</code>, <code>flatten</code>, <code>transpose</code> <code>concatenate</code>, <code>stack</code>, <code>squeeze</code>, <code>expand_dims</code> <code>stride_tricks</code>, <code>as_strided</code>, In-place operations Indexing Basic slicing <code>[0:5]</code> Boolean masking, Fancy indexing <code>np.take</code>, <code>np.put</code>, Advanced stride manipulation Math Ops +, -, *, /, <code>np.sum</code>, <code>np.mean</code> <code>np.dot</code>, <code>np.matmul</code>, Aggregations (<code>axis</code>) <code>np.einsum</code>, Universal Functions (UFuncs), Broadcasting rules Linear Algebra <code>np.linalg.inv</code>, <code>np.linalg.det</code> Eigenvalues, SVD, Matrix Decomposition Sparse matrices (<code>scipy.sparse</code>), BLAS/LAPACK binding Random <code>np.random.rand</code>, <code>np.random.randint</code> <code>np.random.normal</code>, Seeds, Distributions <code>np.random.Generator</code>, PCG64, Parallel RNG Performance Python loops over arrays Vectorization (avoiding loops) Memory layout (C vs Fortran order), Cache locality Interoperability List to Array conversion Array to List, Pandas interop ctypes, C-API, Sharing memory with PyTorch/TensorFlow"},{"location":"month-01/week-01-python-git-linux/python/#3-pandas-data-manipulation","title":"3. Pandas (Data Manipulation)","text":"Topic Area Beginner (Foundations) Intermediate (Engineering) Advanced / Elite (Optimization &amp; Internals) Structures <code>Series</code>, <code>DataFrame</code> creation Index management, Column selection MultiIndex (Hierarchical), Categorical dtype I/O <code>read_csv</code>, <code>to_csv</code>, <code>read_excel</code> <code>read_sql</code>, <code>to_parquet</code>, Chunking (<code>chunksize</code>) Feather/Arrow format, Database connectors, S3 integration Cleaning <code>dropna</code>, <code>fillna</code>, <code>drop_duplicates</code> <code>replace</code>, <code>astype</code>, String methods (<code>str.</code>) Regex extraction, Custom NA handling, Memory reduction Selection <code>loc</code>, <code>iloc</code>, Boolean filtering <code>query</code>, <code>select_dtypes</code>, <code>isin</code> Index alignment, Reindexing, MultiIndex slicing Transformation <code>apply</code>, <code>map</code>, <code>applymap</code> Vectorized operations, <code>assign</code> Custom Accessors, <code>pipe</code>, GroupBy transformations Aggregation <code>sum</code>, <code>mean</code>, <code>count</code> <code>groupby</code>, <code>agg</code>, <code>pivot_table</code> <code>groupby</code> internals, Rolling/Expanding windows, Resampling Merging <code>concat</code>, <code>merge</code> (join) <code>join</code>, <code>combine_first</code>, Append Database-style joins, Overlapping columns, Validation Time Series <code>to_datetime</code>, Basic indexing Date ranges, Frequency conversion, Timezones Business days, Custom offsets, Lag/Lead features Performance Default dtypes Downcasting numerics, Categoricals <code>eval</code>/<code>query</code> engine, Swapping to Polars, Memory profiling"},{"location":"month-01/week-01-python-git-linux/python/#4-scikit-learn-machine-learning","title":"4. Scikit-Learn (Machine Learning)","text":"Topic Area Beginner (Foundations) Intermediate (Engineering) Advanced / Elite (Optimization &amp; Internals) API Standard <code>fit</code>, <code>predict</code>, <code>score</code> <code>fit_transform</code>, <code>inverse_transform</code> Custom Estimators, Transformers, Meta-estimators Preprocessing <code>StandardScaler</code>, <code>MinMaxScaler</code> <code>OneHotEncoder</code>, <code>Imputer</code>, <code>Pipeline</code> Custom Transformers, FeatureUnion, ColumnTransformer Models Linear/Logistic Regression, KNN Decision Trees, Random Forest, SVM Gradient Boosting (HistGradientBoosting), Ensemble voting Validation <code>train_test_split</code> <code>cross_val_score</code>, K-Fold, Stratified K-Fold TimeSeriesSplit, GroupKFold, Nested CV, Leakage prevention Metrics <code>accuracy_score</code>, <code>mean_squared_error</code> Precision/Recall, F1, ROC/AUC, Confusion Matrix Custom scorers, Multi-output metrics, Probability calibration Tuning Manual parameter change <code>GridSearchCV</code>, <code>RandomizedSearchCV</code> <code>HalvingGridSearch</code>, Bayesian Opt (integration), Early stopping Pipeline Basic sequential steps Feature selection within pipeline Parallel pipeline execution, Caching (<code>memory</code> param) Serialization <code>pickle</code> <code>joblib.dump</code>, <code>joblib.load</code> Model versioning, ONNX conversion, Sparse model storage Scaling Single machine, RAM fit <code>partial_fit</code> (Online learning) Distributed strategies, Out-of-core learning, Approximate nearest neighbors"},{"location":"month-01/week-01-python-git-linux/python/jupyter-demo/","title":"Jupyter Notebook Native Support","text":"In\u00a0[1]: Copied! <pre>print(\"Hello, MkDocs-Jupyter! Everything is working correctly.\")\n\ntopics = [\"Neural Networks\", \"Transformers\", \"Agentic Architectures\"]\nprint(\"List of dynamically added AI topics:\")\nfor topic in topics:\n    print(f\"- {topic}\")\n</pre> print(\"Hello, MkDocs-Jupyter! Everything is working correctly.\")  topics = [\"Neural Networks\", \"Transformers\", \"Agentic Architectures\"] print(\"List of dynamically added AI topics:\") for topic in topics:     print(f\"- {topic}\") <pre>Hello, MkDocs-Jupyter! Everything is working correctly.\nList of dynamically added AI topics:\n- Neural Networks\n- Transformers\n- Agentic Architectures\n</pre>"},{"location":"month-01/week-01-python-git-linux/python/jupyter-demo/#jupyter-notebook-native-support","title":"Jupyter Notebook Native Support\u00b6","text":"<p>This is a sample notebook proving that MkDocs can parse, render, and execute Jupyter Notebooks directly inside the academy website natively.</p> <p>You can now securely drop <code>.ipynb</code> files into any nested folder!</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/How_SD_SEM_MoE_CI_Connected/","title":"\ud83c\udfaf SD vs SEM vs MoE vs CI \u2014 Explained with ONE Example","text":"<p>Let's use one scenario and see how all four terms fit in.</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/How_SD_SEM_MoE_CI_Connected/#the-scenario-measuring-glucose-in-diabetes-patients","title":"\ud83c\udfe5 The Scenario: Measuring Glucose in Diabetes Patients","text":"<p>You test 100 patients at a clinic.</p> <p>Your data: - Average (Mean) glucose = 140 mg/dL - Standard Deviation (SD) = 30 mg/dL - Sample size (n) = 100</p> <p>Now let's decode the four terms:</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/How_SD_SEM_MoE_CI_Connected/#1-standard-deviation-sd-how-spread-out-are-the-patients","title":"1\ufe0f\u20e3 Standard Deviation (SD) = \"How spread out are the patients?\"","text":"<p>\ud83d\udde3\ufe0f \"Patients' glucose levels vary by about \u00b130 mg/dL from the average.\"</p> Patient Glucose Distance from Mean (140) Alice 110 -30 Bob 140 0 Charlie 170 +30 Diana 200 +60 <p>SD = 30 means: Most patients (\u224868%) have glucose between 110 and 170.</p> <p>\u2705 Use SD when: You want to describe how different individual people are.</p> <p>\"Glucose levels in our sample ranged widely (Mean=140, SD=30).\"</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/How_SD_SEM_MoE_CI_Connected/#2-standard-error-of-the-mean-sem-how-wobbly-is-my-average","title":"2\ufe0f\u20e3 Standard Error of the Mean (SEM) = \"How wobbly is my average?\"","text":"<p>\ud83d\udde3\ufe0f \"If I repeated this study with 100 new patients, my new average would typically be within \u00b13 mg/dL of 140.\"</p> <p>Formula: <code>SEM = SD / \u221an = 30 / \u221a100 = 30 / 10 = 3</code></p> If you repeated the study... Possible Sample Mean Study #1 138 Study #2 142 Study #3 139 Study #4 141 Typical \"wobble\" \u00b13 \u2190 That's the SEM <p>\u2705 Use SEM when: You're doing math, comparing study precision, or building confidence intervals.</p> <p>\"Our estimate of average glucose has SEM=3, indicating high precision.\"</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/How_SD_SEM_MoE_CI_Connected/#3-margin-of-error-moe-the-number-you-see-in-polls","title":"3\ufe0f\u20e3 Margin of Error (MoE) = \"The \u00b1 number you see in polls\"","text":"<p>\ud83d\udde3\ufe0f \"Our average of 140 could be off by about \u00b16 mg/dL.\"</p> <p>Formula: <code>MoE = Critical Value \u00d7 SEM</code> - For 95% confidence, critical value \u2248 1.96 (often rounded to 2) - <code>MoE = 1.96 \u00d7 3 \u2248 6</code></p> <p>\ud83d\uddf3\ufe0f Real-world example (polling):</p> <p>\"Candidate A leads with 52% support, \u00b13% margin of error.\" \u2192 This \"\u00b13%\" is the MoE.</p> <p>\u2705 Use MoE when: You want a quick, simple uncertainty number for reports or headlines.</p> <p>\"Average glucose: 140 \u00b16 mg/dL\"</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/How_SD_SEM_MoE_CI_Connected/#4-confidence-interval-ci-the-full-range-of-likely-values","title":"4\ufe0f\u20e3 Confidence Interval (CI) = \"The full range of likely values\"","text":"<p>\ud83d\udde3\ufe0f \"We're 95% confident the true average glucose for ALL diabetes patients is between 134 and 146 mg/dL.\"</p> <p>Formula: <code>CI = Mean \u00b1 MoE = 140 \u00b1 6 = [134, 146]</code></p> <p>\ud83c\udfaf Visual: <pre><code>True Population Mean (unknown)\n          \u2502\n          \u25bc\n    [134 \u2500\u2500\u2500\u2500\u2500 140 \u2500\u2500\u2500\u2500\u2500 146]\n          \u2502      \u2502      \u2502\n          \u2502   Our sample \u2502\n          \u2502   mean       \u2502\n          \u2502              \u2502\n    \"We're 95% sure the truth is in this range\"\n</code></pre></p> <p>\u2705 Use CI when: You're reporting results to doctors, researchers, or in papers.</p> <p>\"Mean glucose was 140 mg/dL (95% CI: 134\u2013146).\"</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/How_SD_SEM_MoE_CI_Connected/#how-they-connect-the-recipe","title":"\ud83e\udde9 How They Connect (The Recipe)","text":"<pre><code>1. Start with your data \u2192 Calculate SD (how spread out patients are)\n2. Divide by \u221an \u2192 Get SEM (how wobbly your average is)\n3. Multiply by 1.96 \u2192 Get MoE (the \u00b1 number for 95% confidence)\n4. Add/Subtract from mean \u2192 Get CI (the final range to report)\n</code></pre> <p>In our example: <pre><code>SD = 30\n \u2193\nSEM = 30 / \u221a100 = 3\n \u2193\nMoE = 1.96 \u00d7 3 \u2248 6\n \u2193\nCI = 140 \u00b1 6 = [134, 146]\n</code></pre></p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/How_SD_SEM_MoE_CI_Connected/#quick-comparison-table","title":"\ud83d\udcca Quick Comparison Table","text":"Term Answers the Question... Formula Example Result When to Report SD How different are the patients? <code>\u221a[\u03a3(x-mean)\u00b2/(n-1)]</code> <code>30 mg/dL</code> Describing your sample data SEM How precise is my average? <code>SD / \u221an</code> <code>3 mg/dL</code> Internal calculations, comparing studies MoE What's the \"\u00b1\" for my guess? <code>1.96 \u00d7 SEM</code> <code>\u00b16 mg/dL</code> Polls, quick summaries, headlines CI What range likely contains the truth? <code>Mean \u00b1 MoE</code> <code>[134, 146]</code> Research papers, medical reports, presentations"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/How_SD_SEM_MoE_CI_Connected/#another-example-diabetes-prevalence-binary-outcome","title":"\ud83c\udfb2 Another Example: Diabetes Prevalence (Binary Outcome)","text":"<p>Same 100 patients. <code>outcome</code>: 0 = No Diabetes, 1 = Diabetes.</p> <p>Your data: - 35 patients have diabetes \u2192 Mean = 0.35 (35% prevalence) - SD of binary data = 0.48 (formula: <code>\u221a[p(1-p)]</code>) - n = 100</p> Term Calculation Result Layman Translation SD <code>\u221a[0.35\u00d70.65]</code> <code>0.48</code> \"Individual patients vary a lot: some have diabetes, some don't.\" SEM <code>0.48 / \u221a100</code> <code>0.048</code> \"If we re-sampled, our prevalence estimate would wobble by ~\u00b15 percentage points.\" MoE <code>1.96 \u00d7 0.048</code> <code>\u00b10.094</code> \"Our 35% estimate could be off by about \u00b19 percentage points.\" CI <code>0.35 \u00b1 0.094</code> <code>[25.6%, 44.4%]</code> \"We're 95% confident the true diabetes rate in the population is between 26% and 44%.\" <p>\ud83d\udde3\ufe0f Report to a non-technical audience:</p> <p>\"In our sample, 35% of patients had diabetes. The true rate in the wider population is likely between 26% and 44%.\" (You just reported the CI. The SEM and MoE did the heavy lifting behind the scenes.)</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/How_SD_SEM_MoE_CI_Connected/#when-to-use-which-decision-guide","title":"\ud83e\udded When to Use Which: Decision Guide","text":"<pre><code>Are you describing your sample data?\n\u251c\u2500 Yes \u2192 Report MEAN + SD\n\u2502   Example: \"Glucose: 140 \u00b1 30 mg/dL\"\n\u2502\n\u2514\u2500 No \u2192 Are you reporting uncertainty about an estimate?\n    \u251c\u2500 Yes \u2192 Talking to non-experts?\n    \u2502   \u251c\u2500 Yes \u2192 Report CONFIDENCE INTERVAL\n    \u2502   \u2502   Example: \"140 mg/dL (95% CI: 134\u2013146)\"\n    \u2502   \u2502\n    \u2502   \u2514\u2500 No (quick headline) \u2192 Report MARGIN OF ERROR\n    \u2502       Example: \"140 \u00b1 6 mg/dL\"\n    \u2502\n    \u2514\u2500 No \u2192 Doing calculations or comparing studies?\n        \u2514\u2500 Yes \u2192 Use STANDARD ERROR (SEM) internally\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/How_SD_SEM_MoE_CI_Connected/#one-final-analogy-ordering-pizza","title":"\ud83c\udf55 One Final Analogy: Ordering Pizza","text":"Term Pizza Analogy SD \"Pizzas in this city vary widely: some small, some large.\" SEM \"If I order 10 more pizzas, my average size estimate won't change much.\" MoE \"My guess of the average size is \u00b11 inch.\" CI \"I'm 95% sure the true average pizza size in this city is between 11 and 13 inches.\" <p>\ud83c\udf55 You serve the CI (the full box). SEM and MoE are the kitchen tools that made it.</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/How_SD_SEM_MoE_CI_Connected/#quick-cheat-sheet","title":"\u2705 Quick Cheat Sheet","text":"<pre><code># Given: mean=140, sd=30, n=100\nimport scipy.stats as st\n\nsem = 30 / (100**0.5)           # 3.0\nmoe = 1.96 * sem                # 5.88\nci = (140 - moe, 140 + moe)     # (134.12, 145.88)\n\n# Or use scipy directly:\nci_direct = st.t.interval(0.95, df=99, loc=140, scale=sem)\n</code></pre> You Want To... Use This Report This Show data spread <code>df['glucose'].std()</code> \"Mean = 140, SD = 30\" Compare study precision <code>st.sem(df['glucose'])</code> (Keep internal) Quick uncertainty headline <code>1.96 * sem</code> \"140 \u00b1 6 mg/dL\" Formal research report <code>st.t.interval(...)</code> \"140 (95% CI: 134\u2013146)\" <p>Bottom line:  - SD = Patient variability \ud83d\udcca - SEM = Estimate precision \ud83c\udfaf - MoE = The \"\u00b1\" number \u2795\u2796 - CI = The final answer range \ud83c\udf81</p> <p>You need all four, but you report the one your audience will understand best. \ud83d\ude80</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/Z_score_%26_Standardization/","title":"\ud83c\udf93 Z-Score vs Standard Deviation: College Grades Example","text":"<p>Let's use a concrete example to make this crystal clear.</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/Z_score_%26_Standardization/#the-scenario-final-exam-in-intro-to-statistics","title":"\ud83d\udcca The Scenario: Final Exam in \"Intro to Statistics\"","text":"<p>Class ** - 200 students took the exam - Mean (average) = 75 out of 100 - Standard Deviation **(SD) = 10 points</p> <p>Your score: 90 out of 100</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/Z_score_%26_Standardization/#what-does-standard-deviation-sd-10-tell-us","title":"\ud83d\udd39 What Does Standard Deviation (SD = 10) Tell Us?","text":"<p>\ud83d\udde3\ufe0f \"Most students' scores are within \u00b110 points of the average (75).\"</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/Z_score_%26_Standardization/#visual-the-grade-distribution","title":"Visual: The Grade Distribution","text":"<pre><code>Score:  55   65   75   85   95   105\n        \u2502    \u2502    \u2502    \u2502    \u2502    \u2502\n        \u2502    \u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588    \u2502\n        \u2502  \u2588\u2588                \u2588\u2588    \u2502\n        \u2502\u2588\u2588                    \u2588\u2588  \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192\n              Most students (\u224868%)\n              scored between 65\u201385\n</code></pre> <p>SD = 10 means: | Range | Score Range | % of Students | Interpretation | |-------|------------|---------------|---------------| | Mean \u00b1 1 SD | 65 to 85 | ~68% | \"Typical\" students | | Mean \u00b1 2 SD | 55 to 95 | ~95% | \"Almost everyone\" | | Mean \u00b1 3 SD | 45 to 105 | ~99.7% | \"Virtually all\" |</p> <p>\u2705 SD describes the CLASS: \"This class had moderate variability \u2014 scores typically varied by about 10 points from the average.\"</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/Z_score_%26_Standardization/#what-does-z-score-15-tell-us","title":"\ud83d\udd39 What Does Z-Score = 1.5 Tell Us?","text":"<p>\ud83d\udde3\ufe0f \"Your score is 1.5 standard deviations ABOVE the class average.\"</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/Z_score_%26_Standardization/#the-formula-simple-version","title":"The Formula (Simple Version):","text":"<pre><code>Z = (Your Score - Class Mean) / SD\nZ = (90 - 75) / 10 = 15 / 10 = 1.5 \u2705\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/Z_score_%26_Standardization/#what-z-15-actually-means","title":"What Z = 1.5 Actually Means:","text":"Z-Score Interpretation Percentile* Grade Context Z = 0 Exactly average 50th percentile Solid C+/B- Z = +1.0 1 SD above average ~84th percentile Good B+/A- Z = +1.5 1.5 SD above average ~93rd percentile Strong A Z = +2.0 2 SD above average ~98th percentile Top of class Z = -1.0 1 SD below average ~16th percentile Struggling <p>* Assuming normal distribution</p> <p>\u2705 Z-score describes YOU relative to the class: \"You scored better than approximately 93% of your classmates.\"</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/Z_score_%26_Standardization/#side-by-side-comparison","title":"\ud83e\udde9 Side-by-Side Comparison","text":"Question SD Answers Z-Score Answers What is it? A property of the entire dataset A property of one data point relative to the dataset Units Same as original data (points, dollars, etc.) Unitless (standardized) Formula <code>\u221a[\u03a3(x-mean)\u00b2/(n-1)]</code> <code>(x - mean) / SD</code> In our example \"Scores vary by ~10 points\" \"You are 1.5 'score-units' above average\" Use case Describing class performance spread Comparing your performance across different exams"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/Z_score_%26_Standardization/#why-z-scores-are-powerful-cross-class-comparison","title":"\ud83c\udfaf Why Z-Scores Are Powerful: Cross-Class Comparison","text":""},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/Z_score_%26_Standardization/#scenario-you-took-two-exams","title":"Scenario: You took two exams","text":"Exam Your Score Class Mean Class SD Your Z-Score Statistics 90 75 10 (90-75)/10 = +1.5 Calculus 82 70 4 (82-70)/4 = +3.0 <p>Question: \"Which exam did you do better on, relative to your classmates?\"</p> <p>\u274c Wrong answer: \"Statistics! 90 &gt; 82\" \u2705 Right answer: \"Calculus! Z = +3.0 is more exceptional than Z = +1.5\"</p> <p>Interpretation: - Statistics: You beat ~93% of classmates - Calculus: You beat ~99.9% of classmates \ud83c\udfc6</p> <p>\ud83d\udde3\ufe0f Z-scores let you compare apples to oranges by putting everything on the same \"standardized\" scale.</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/Z_score_%26_Standardization/#visual-where-z-15-lands","title":"\ud83d\udcc8 Visual: Where Z = 1.5 Lands","text":"<pre><code>Class Grade Distribution (Normal Curve)\n\n        ^\n        |           ***\nPercent |         **   **\nof      |        *       *\nStudents|       *    \u2191    *\n        |      *   Z=1.5  *\n        |     *     |     *\n        |    *      |90pts*\n        |___*_______|_____*______&gt; Score\n           55  65  75  85  95  105\n               \u2191   \u2191   \u2191\n              -1SD Mean +1SD\n\nShaded area to the LEFT of Z=1.5 = ~93% of students\n\u2192 You scored higher than 93% of the class\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/Z_score_%26_Standardization/#quick-reference-z-score-to-percentile","title":"\ud83e\uddee Quick Reference: Z-Score to Percentile","text":"Z-Score Percentile Layman Translation -3.0 0.1% Extremely low -2.0 2.3% Very low -1.0 16% Below average 0 50% Exactly average +1.0 84% Above average +1.5 93% Strong performance +2.0 98% Excellent +3.0 99.9% Exceptional <p>(Based on standard normal distribution)</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/Z_score_%26_Standardization/#practical-interview-answer","title":"\ud83d\udca1 Practical Interview Answer","text":"<p>Q: \"A student has a Z-score of 1.5 on an exam. What does that mean?\"</p> <p>\u2705 Strong Answer:</p> <p>\"A Z-score of 1.5 means the student scored 1.5 standard deviations above the class mean. </p> <p>For example, if the class average was 75 with a standard deviation of 10, this student scored 90. </p> <p>Assuming a normal distribution, this places them at approximately the 93rd percentile \u2014 they performed better than about 93% of their classmates.</p> <p>Z-scores are useful because they're unitless, allowing comparison across different exams or metrics.\"</p> <p>\ud83c\udfaf Elite Bonus:</p> <p>\"In ML, we use Z-score standardization (<code>(x - mean)/std</code>) to put features on the same scale \u2014 critical for algorithms like SVM, KNN, and neural networks that are sensitive to feature magnitude.\"</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/Z_score_%26_Standardization/#quick-decision-guide","title":"\ud83e\udded Quick Decision Guide","text":"<pre><code>Do you want to describe...?\n\u2502\n\u251c\u2500 The spread of the ENTIRE dataset?\n\u2502  \u2514\u2500 \u2192 Report STANDARD DEVIATION (SD)\n\u2502     Example: \"Exam scores had SD = 10 points\"\n\u2502\n\u2514\u2500 Where ONE data point sits relative to the dataset?\n   \u2514\u2500 \u2192 Calculate Z-SCORE\n      Example: \"Your Z = 1.5 \u2192 top 7% of class\"\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/Z_score_%26_Standardization/#bottom-line","title":"\u2705 Bottom Line","text":"Metric Answers Example (Grades) SD \"How spread out is the class?\" \"Scores typically vary by \u00b110 points\" Z-Score \"How exceptional is this student?\" \"You're 1.5 SD above average \u2192 ~93rd percentile\" <p>\ud83c\udf93 SD describes the crowd. Z-score describes your place in it.</p> <p>Your Z = 1.5? You didn't just pass \u2014 you excelled. \ud83c\udf1f</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/Z_score_%26_Standardization/#standardization-z-score-normalization","title":"Standardization (Z-Score Normalization)","text":""},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/Z_score_%26_Standardization/#1-definition","title":"1. Definition","text":"<p>Standardization is a preprocessing technique that rescales features so they have a mean of 0 and a standard deviation of 1. It transforms data into a standard normal distribution.</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/Z_score_%26_Standardization/#2-the-formula","title":"2. The Formula","text":"<p>For every value \\(x\\) in a feature column:</p> \\[z = \\frac{x - \\mu}{\\sigma}\\] <p>Where: *   \\(x\\) = Original value *   \\(\\mu\\) = Mean of the feature *   \\(\\sigma\\) = Standard deviation of the feature *   \\(z\\) = New standardized value</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/Z_score_%26_Standardization/#3-what-it-does-to-data","title":"3. What It Does to Data","text":"Metric Before Standardization After Standardization Mean Any number (e.g., 150.5) 0 Std Dev Any number (e.g., 30.2) 1 Distribution Shape Unchanged (skewness/kurtosis remain same) Unchanged (only shifted and scaled) Units Original units (e.g., mg/dL, dollars) Unitless (standard deviations from mean)"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/Z_score_%26_Standardization/#4-concrete-example-diabetes-dataset","title":"4. Concrete Example: Diabetes Dataset","text":"<p>Original Data (<code>glucose</code> column): *   Mean (\\(\\mu\\)): 120 *   Std Dev (\\(\\sigma\\)): 30 *   Values: <code>[90, 120, 150]</code></p> <p>Calculation: 1.  Value 90: \\((90 - 120) / 30 = -30 / 30 = \\mathbf{-1.0}\\)     *   Interpretation: 1 standard deviation below average. 2.  Value 120: \\((120 - 120) / 30 = 0 / 30 = \\mathbf{0.0}\\)     *   Interpretation: Exactly average. 3.  Value 150: \\((150 - 120) / 30 = 30 / 30 = \\mathbf{+1.0}\\)     *   Interpretation: 1 standard deviation above average.</p> <p>Resulting Data: <code>[-1.0, 0.0, 1.0]</code></p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/Z_score_%26_Standardization/#5-why-it-is-essential-for-ml","title":"5. Why It Is Essential for ML","text":"<p>A. Distance-Based Algorithms (KNN, K-Means, SVM) These algorithms calculate distance between points (e.g., Euclidean distance). *   Without Standardization: A feature with large values (e.g., <code>Income</code> in thousands) dominates the distance calculation over a feature with small values (e.g., <code>Age</code> in tens). The model effectively ignores the small-scale feature. *   With Standardization: All features contribute equally to the distance because they are on the same scale (units of standard deviation).</p> <p>B. Gradient Descent Optimization (Linear Regression, Neural Networks) *   Without Standardization: The loss function landscape is elongated (like a narrow canyon). The optimizer takes tiny, zig-zagging steps, making training very slow. *   With Standardization: The loss landscape becomes spherical (like a bowl). The optimizer moves directly to the minimum, converging significantly faster.</p> <p>C. Regularization (L1/L2) *   Regularization penalizes large coefficients. If features are on different scales, the penalty is applied unfairly. Standardization ensures the penalty treats all features equally.</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/Z_score_%26_Standardization/#6-python-implementation-scikit-learn","title":"6. Python Implementation (Scikit-Learn)","text":"<pre><code>from sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# Sample data: [Age, Income]\n# Age is small (20-60), Income is large (30k-100k)\nX = np.array([\n    [25, 30000],\n    [45, 60000],\n    [60, 90000]\n])\n\n# Initialize and fit\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nprint(\"Original Mean:\", X.mean(axis=0))      # [43.33, 60000.0]\nprint(\"Scaled Mean:  \", X_scaled.mean(axis=0)) # [0.0, 0.0] (Approx due to float precision)\n\nprint(\"Original Std: \", X.std(axis=0))       # [14.3, 24494.9]\nprint(\"Scaled Std:   \", X_scaled.std(axis=0))  # [1.0, 1.0]\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/Z_score_%26_Standardization/#7-when-not-to-use-it","title":"7. When NOT to Use It","text":"<ul> <li>Tree-Based Models: Decision Trees, Random Forests, and XGBoost split data based on thresholds, not distance. They are invariant to scale. Standardization provides no benefit here.</li> <li>Sparse Data: If you have many zeros (sparse matrices), standardization can destroy sparsity by centering the data (making zeros non-zero), which increases memory usage.</li> <li>Outliers: Since the mean and std dev are sensitive to outliers, extreme values can squash the rest of the data into a tiny range. In such cases, Robust Scaling (using median and IQR) is preferred.</li> </ul>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/Z_score_%26_Standardization/#summary-checklist","title":"Summary Checklist","text":"<ul> <li>Goal: Make features comparable (mean=0, std=1).</li> <li>Use For: Linear models, Neural Networks, KNN, K-Means, PCA, SVM.</li> <li>Skip For: Tree-based models (Random Forest, XGBoost).</li> <li>Caution: Check for outliers before applying.</li> </ul>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/case_study_DiGiorno/","title":"case study DiGiorno","text":""},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/case_study_DiGiorno/#module-inferential-statistics-uncertainty","title":"Module: Inferential Statistics &amp; Uncertainty","text":""},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/case_study_DiGiorno/#topic-confidence-intervals-ci","title":"Topic: Confidence Intervals (CI)","text":"<p>Professional Context: In a production ML environment, a single prediction (Point Estimate) is dangerous without a measure of uncertainty. If an AI predicts a 10% churn rate, but the 95% CI is \\([2\\%, 40\\%]\\), the model is too unstable for business decisions.</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/case_study_DiGiorno/#1-the-conceptual-framework","title":"1. The Conceptual Framework","text":"<p>A Confidence Interval is an estimated range of values which is likely to include an unknown population parameter. The width of the interval tells us about the precision of our estimate.</p> <ul> <li>The Goal: Move from \"What is the number?\" to \"How sure are we about this range?\"</li> <li>The Analogy: If you are an archer, the Point Estimate is where your arrow lands. The Confidence Interval is the size of the target you are 95% sure you can hit every time.</li> </ul>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/case_study_DiGiorno/#2-mathematical-intuition","title":"2. Mathematical Intuition","text":"<p>To calculate a Confidence Interval for a mean (\\(\\mu\\)), we use the formula:</p> \\[\\text{CI} = \\bar{x} \\pm \\left( z^* \\cdot \\frac{\\sigma}{\\sqrt{n}} \\right)\\]"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/case_study_DiGiorno/#the-anatomy-of-the-formula","title":"The Anatomy of the Formula:","text":"<ol> <li>\\(\\bar{x}\\) (Sample Mean): Our \"Best Guess\" based on current data.</li> <li>\\(z^*\\) (Critical Value): Determined by your desired confidence level.</li> <li>For 90% Confidence: \\(z^* \\approx 1.645\\)</li> <li>For 95% Confidence: \\(z^* \\approx 1.96\\)</li> <li> <p>For 99% Confidence: \\(z^* \\approx 2.576\\)</p> </li> <li> <p>\\(\\frac{\\sigma}{\\sqrt{n}}\\) (Standard Error): This represents the standard deviation of the sampling distribution.</p> </li> <li>Crucial Insight: As your sample size (\\(n\\)) increases, the Standard Error decreases, making your interval narrower (more precise).</li> </ol>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/case_study_DiGiorno/#3-case-study-the-digiorno-ai-joy-analysis","title":"3. Case Study: The DiGiorno AI \"Joy\" Analysis","text":"<p>In the 2017 facial recognition experiment, DiGiorno measured \"Joy Scores\" (\\(0\\text{--}4\\)).</p> <p>The Data:</p> <ul> <li>Sample size (\\(n\\)): \\(100\\) party guests.</li> <li>Mean Joy Score (\\(\\bar{x}\\)): \\(2.8\\)</li> <li>Std Dev (\\(\\sigma\\)): \\(0.5\\)</li> </ul> <p>The 95% CI Calculation:</p> \\[\\text{Margin of Error} = 1.96 \\cdot \\frac{0.5}{\\sqrt{100}} = 1.96 \\cdot 0.05 = 0.098\\] \\[\\text{95\\% CI} = [2.702, 2.898]\\] <p>Professional Interpretation: \"We are 95% confident that the true average joy score of all customers lies between 2.70 and 2.90. Because this range is narrow and far above the 'neutral' score of 2.0, we can statistically validate the product's emotional impact.\"</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/case_study_DiGiorno/#4-implementation-pythonscipy","title":"4. Implementation (Python/SciPy)","text":"<p>Professional data scientists use <code>scipy.stats</code> for these calculations to ensure precision.</p> <pre><code>import numpy as np\nfrom scipy import stats\n\ndata = [2.1, 2.5, 2.8, 3.0, 2.4, 2.9, 2.7, 3.1, 2.6, 2.8] # Joy Scores\nconfidence = 0.95\n\nmean = np.mean(data)\nsem = stats.sem(data) # Standard Error of the Mean\n\n# Calculate interval\ninterval = stats.t.interval(confidence, len(data)-1, loc=mean, scale=sem)\n\nprint(f\"95% Confidence Interval: {interval}\")\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/case_study_DiGiorno/#assessment-mastery-check","title":"\ud83e\uddea Assessment: Mastery Check","text":"<p>To move to the next module, you must be able to answer:</p> <ol> <li>Scenario: If you increase your confidence level from 95% to 99%, does the interval get wider or narrower? Why?</li> <li>Calculation: If a model predicts a delivery time mean of 30 mins with a Margin of Error of 5, what is the Confidence Interval?</li> <li>Logic: Why is a narrow Confidence Interval usually preferred in manufacturing (like pizza production)?</li> </ol>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/coefficient_of_variation/","title":"Coefficient of variation","text":"<p>The Coefficient of Variation (CV) is the \"Standardization King\" of statistics. While Standard Deviation tells you how much data spreads in absolute terms (e.g., \\(\\$500\\) or \\(10\\text{ cm}\\)), the CV tells you how much it spreads relative to the mean.</p> <p>In our academy, this should be taught as the tool for comparing apples to oranges.</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/coefficient_of_variation/#module-relative-variability","title":"Module: Relative Variability","text":""},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/coefficient_of_variation/#topic-coefficient-of-variation-cv","title":"Topic: Coefficient of Variation (CV)","text":"<p>Professional Context: In Machine Learning, CV is used to assess feature scaling needs and model stability. If you are comparing the volatility of Bitcoin (priced in thousands) vs. a Penny Stock (priced in cents), Standard Deviation will mislead you. CV is the only way to see which is truly more \"volatile.\"</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/coefficient_of_variation/#1-the-conceptual-framework","title":"1. The Conceptual Framework","text":"<p>The Coefficient of Variation expresses the Standard Deviation as a percentage of the Mean. It is a dimensionless number, meaning it has no units (no grams, no dollars, no meters).</p> <ul> <li>High CV: Indicates high relative variability (the data is \"all over the place\" compared to its average).</li> <li>Low CV: Indicates high consistency (the data stays very close to its average).</li> </ul>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/coefficient_of_variation/#2-mathematical-intuition","title":"2. Mathematical Intuition","text":"<p>The formula is elegantly simple:</p> \\[CV = \\left( \\frac{\\sigma}{\\mu} \\right) \\times 100\\%\\] <p>Where:</p> <ul> <li>\\(\\sigma\\) (Sigma): Standard Deviation</li> <li>\\(\\mu\\) (Mu): Population Mean</li> </ul>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/coefficient_of_variation/#why-do-we-divide-by-the-mean","title":"Why do we divide by the Mean?","text":"<p>Imagine two pizza chains:</p> <ol> <li>Chain A: Average delivery time is 10 minutes, Std Dev is 2 minutes.</li> <li>Chain B: Average delivery time is 60 minutes, Std Dev is 2 minutes.</li> </ol> <p>Both have the same \"spread\" (\\(2\\text{ mins}\\)), but for Chain A, a \\(2\\text{-minute}\\) delay is a 20% variation. For Chain B, it\u2019s only a 3.3% variation. Chain B is technically much more consistent relative to its operation.</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/coefficient_of_variation/#3-case-study-digiorno-vs-competitor-consistency","title":"3. Case Study: DiGiorno vs. Competitor Consistency","text":"<p>DiGiorno wants to prove their production line is more \"reliable\" than a local pizzeria.</p> Metric DiGiorno (Factory) Local Pizzeria (Hand-made) Mean Weight \\(800\\text{g}\\) \\(950\\text{g}\\) Std Dev (\\(\\sigma\\)) \\(8\\text{g}\\) \\(45\\text{g}\\) CV calculation \\((8 / 800) \\times 100 = \\mathbf{1\\%}\\) \\((45 / 950) \\times 100 = \\mathbf{4.7\\%}\\) <p>Professional Interpretation: Even though the local pizza is heavier, DiGiorno has a much lower CV (\\(1\\%\\)). This proves process stability. In Data Science, we look for low CV in our model's cross-validation scores to ensure the model isn't just \"getting lucky\" on certain folds.</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/coefficient_of_variation/#4-implementation-pythonpandas","title":"4. Implementation (Python/Pandas)","text":"<p>In real-world data science, we often calculate CV across multiple columns to decide which features need normalization.</p> <pre><code>import pandas as pd\n\n# Sample Data: Price vs. Quantity Sold\ndata = {\n    'Price_USD': [100, 102, 98, 105, 95],\n    'Units_Sold': [1000, 1500, 800, 2000, 500]\n}\ndf = pd.DataFrame(data)\n\n# CV function: (std / mean)\ncv = df.std() / df.mean()\n\nprint(\"Coefficient of Variation:\")\nprint(cv)\n\n# Insight: If CV &gt; 1 (or 100%), the data is considered high-variance.\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/coefficient_of_variation/#assessment-mastery-check","title":"\ud83e\uddea Assessment: Mastery Check","text":"<ol> <li>Conceptual: If a dataset has a Mean of \\(0\\), why can't we calculate the CV? (Answer: Division by zero).</li> <li>Business Logic: You are comparing the \"Joy Scores\" of two different pizza ad campaigns. Campaign A has a mean of \\(2.0\\) with \\(\\sigma = 0.2\\). Campaign B has a mean of \\(3.5\\) with \\(\\sigma = 0.4\\). Which campaign is more consistent in its impact?</li> <li>Units: If the mean is in \"Kilograms,\" what is the unit of the Coefficient of Variation? (Answer: None/Percentage).</li> </ol>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/covariance_vs_corelation/","title":"Covariance vs corelation","text":"<p>To teach Covariance vs. Correlation effectively in your academy, you must highlight that one is a raw measure of direction, while the other is a standardized measure of strength.</p> <p>In ML, this is the foundation of Feature Selection and Principal Component Analysis (PCA).</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/covariance_vs_corelation/#module-relationship-metrics","title":"Module: Relationship Metrics","text":""},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/covariance_vs_corelation/#topic-covariance-vs-correlation","title":"Topic: Covariance vs. Correlation","text":"<p>Professional Context: In the exploratory data analysis (EDA) phase, we use these to identify redundant features. If two variables are perfectly correlated (\\(1.0\\)), we can often drop one to reduce model complexity and prevent Multicollinearity.</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/covariance_vs_corelation/#1-the-conceptual-framework","title":"1. The Conceptual Framework","text":"<p>Both metrics tell you how two variables move together, but they speak different languages:</p> <ul> <li>Covariance: Tells you the Direction of the relationship. (Positive, Negative, or Zero). It is tied to the original units of the data (e.g., \\(\\$ \\times \\text{kg}\\)).</li> <li>Correlation (Pearson\u2019s \\(r\\)): Tells you both the Direction AND the Strength. It is a \"scaled\" version of covariance that always stays between \\(-1\\) and \\(+1\\).</li> </ul>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/covariance_vs_corelation/#2-mathematical-intuition","title":"2. Mathematical Intuition","text":""},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/covariance_vs_corelation/#the-formula-for-covariance-textcov_xy","title":"The Formula for Covariance (\\(\\text{cov}_{x,y}\\)):","text":"\\[\\text{cov}(x,y) = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1}\\] <ul> <li>If \\(x\\) and \\(y\\) both increase together, the product is positive.</li> <li>The Problem: If you change the units (e.g., from meters to centimeters), the covariance value changes drastically, even though the relationship hasn't changed.</li> </ul>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/covariance_vs_corelation/#the-formula-for-correlation-r","title":"The Formula for Correlation (\\(r\\)):","text":"\\[r = \\frac{\\text{cov}(x,y)}{\\sigma_x \\sigma_y}\\] <ul> <li>By dividing the covariance by the product of the standard deviations, we standardize the value.</li> <li>The Benefit: It is unitless. Whether you measure in miles or kilometers, the correlation remains identical.</li> </ul>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/covariance_vs_corelation/#3-case-study-pizza-price-vs-joy-score","title":"3. Case Study: Pizza Price vs. \"Joy Score\"","text":"<p>Imagine you are analyzing data for your DiGiorno study.</p> Metric Covariance Result Correlation (\\(r\\)) Result Value \\(145.2\\) \\(0.85\\) Interpretation \"There is a positive relationship.\" \"There is a strong positive relationship.\" Scalability Hard to compare with other data. Easy to compare with \"Delivery Time\" vs \"Joy.\" <p>Strategic Insight: If you see a Correlation of \\(0.85\\), you know the variables are tightly linked. A Covariance of \\(145.2\\) means nothing without knowing the scale of the original data.</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/covariance_vs_corelation/#4-implementation-pythonseaborn","title":"4. Implementation (Python/Seaborn)","text":"<p>In a professional setting, we visualize this using a Heatmap.</p> <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Sample Data\ndata = {'Price': [10, 12, 15, 20, 25], \n        'Joy_Score': [2.1, 2.5, 3.0, 3.8, 4.0],\n        'Delivery_Time': [30, 25, 20, 15, 10]}\ndf = pd.DataFrame(data)\n\n# 1. Covariance Matrix\nprint(df.cov())\n\n# 2. Correlation Matrix (Most common in ML)\ncorr_matrix = df.corr()\nprint(corr_matrix)\n\n# 3. Visualization\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\nplt.title(\"Feature Correlation Heatmap\")\nplt.show()\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/covariance_vs_corelation/#assessment-mastery-check","title":"\ud83e\uddea Assessment: Mastery Check","text":"<ol> <li>True or False: If the covariance between two variables is \\(0\\), they are not linearly related. (Answer: True).</li> <li>Logic: Why is the correlation between \"Price in Dollars\" and \"Joy\" the same as \"Price in Cents\" and \"Joy\"? (Answer: Correlation is standardized/unitless).</li> <li>ML Application: If two features have a correlation of \\(0.99\\), why might you remove one before training a Linear Regression model? (Answer: To avoid Multicollinearity).</li> </ol>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/standard_error_of_mean/","title":"Standard error of mean","text":"<p>To round out your Statistics foundation, the Standard Error of the Mean (SEM) is the most critical metric for moving from \"Data Analysis\" to \"Scientific Inference.\"</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/standard_error_of_mean/#topic-standard-error-of-the-mean-sem","title":"Topic: Standard Error of the Mean (SEM)","text":"<p>Professional Context: In A/B testing or model evaluation, we don't just care about the mean; we care about how much that mean would jump around if we ran the experiment again. The SEM quantifies this \"jitter.\"</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/standard_error_of_mean/#1-the-conceptual-framework","title":"1. The Conceptual Framework","text":"<p>The Standard Error (SEM) measures the dispersion of sample means around the true population mean.</p> <ul> <li>Standard Deviation (\\(\\sigma\\)): Tells you how much individual data points (e.g., individual pizza weights) vary from the mean.</li> <li>Standard Error (SEM): Tells you how much the average of a sample (e.g., the average weight of 100 pizzas) would vary if you took a different sample of 100 pizzas.</li> </ul>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/standard_error_of_mean/#2-mathematical-intuition","title":"2. Mathematical Intuition","text":"<p>The formula for SEM is derived from the Central Limit Theorem:</p> \\[\\text{SEM} = \\frac{\\sigma}{\\sqrt{n}}\\] <p>Where:</p> <ul> <li>\\(\\sigma\\) = Standard Deviation of the population (or sample).</li> <li>\\(n\\) = Number of observations in the sample.</li> </ul>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/standard_error_of_mean/#the-law-of-large-numbers-insight","title":"The \"Law of Large Numbers\" Insight:","text":"<p>As \\(n\\) (sample size) increases, the denominator gets larger, making the SEM smaller.</p> <ul> <li>Translation: The more data you collect, the more \"certain\" your average becomes. A mean based on 1,000 people is much more stable than a mean based on 5 people.</li> </ul>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/standard_error_of_mean/#3-case-study-digiorno-quality-control","title":"3. Case Study: DiGiorno Quality Control","text":"<p>DiGiorno wants to ensure every batch of dough is consistent.</p> <ul> <li>Batch A (\\(n=10\\)): \\(\\bar{x} = 500\\text{g}\\), \\(\\sigma = 10\\text{g}\\).</li> <li>Batch B (\\(n=100\\)): \\(\\bar{x} = 500\\text{g}\\), \\(\\sigma = 10\\text{g}\\).</li> </ul> <p>SEM Calculation:</p> <ul> <li>Batch A SEM: \\(10 / \\sqrt{10} \\approx \\mathbf{3.16}\\)</li> <li>Batch B SEM: \\(10 / \\sqrt{100} = \\mathbf{1.00}\\)</li> </ul> <p>Professional Interpretation: Even though the \"spread\" (\\(\\sigma\\)) is the same, we are 3 times more confident in the mean of Batch B. If we tested another 100 pizzas, the new mean would likely be within \\(1\\text{g}\\) of the first one.</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/standard_error_of_mean/#4-implementation-pythonnumpy","title":"4. Implementation (Python/NumPy)","text":"<pre><code>import numpy as np\nfrom scipy import stats\n\ndata = [502, 498, 505, 495, 500, 501, 499]\n\n# Method 1: Manual\nsem_manual = np.std(data, ddof=1) / np.sqrt(len(data))\n\n# Method 2: SciPy (Standard Way)\nsem_scipy = stats.sem(data)\n\nprint(f\"Standard Error of the Mean: {sem_scipy:.4f}\")\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/standard_error_of_mean/#the-interview-pass-definition","title":"\ud83c\udfaf The \"Interview Pass\" Definition","text":"<p>If an interviewer asks, \"What is the Standard Error?\", use this 3-part structure to demonstrate seniority:</p> <p>The Definition: \"The Standard Error is the standard deviation of the sampling distribution of a statistic\u2014most commonly the mean. It quantifies the uncertainty of our estimate.\" The Distinction: \"Unlike Standard Deviation, which describes the variability within a single dataset, the Standard Error describes the variability of the sample mean if we were to repeat the experiment multiple times.\" The Application: \"In practice, I use SEM to calculate Confidence Intervals and \\(p\\text{-values}\\) during hypothesis testing. It tells me if the results I'm seeing in my ML model's performance are statistically significant or just due to sampling noise.\"</p>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/standard_error_of_mean/#assessment-mastery-check","title":"\ud83e\uddea Assessment: Mastery Check","text":"<ol> <li>Conceptual: If you quadruple (\\(4\\times\\)) your sample size, what happens to your Standard Error? (Answer: It is cut in half, because \\(\\sqrt{4} = 2\\)).</li> <li>Logic: Why is SEM always smaller than (or equal to) the Standard Deviation? (Answer: Because it is divided by \\(\\sqrt{n}\\), and \\(n\\) is usually \\(&gt;1\\)).</li> <li>Visual: On a graph, if the \"Error Bars\" (representing SEM) of two groups do not overlap, what does that usually suggest? (Answer: The difference between the groups is likely statistically significant).</li> </ol>"},{"location":"month-01/week-02-statistics-probability/confidence_interval_sem_zscore_standardization_etc/standard_error_of_mean/#standard-deviation-vs-standard-error-of-mean-sem","title":"Standard deviation vs Standard Error of Mean (SEM)","text":"<ul> <li> <p>Standard Deviation: The Standard Deviation is the degree to which the elements within the sample differ from the mean.</p> </li> <li> <p>SEM: The \"SEM\" compares sample mean versus population mean</p> </li> </ul>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/","title":"Descriptive Statistics Essentials Index (0 \u2192 Elite)","text":"Topic Area Beginner (Foundations) Intermediate (Engineering) Advanced / Elite (Optimization &amp; Internals) Central Tendency Mean, Median, Mode Weighted mean, Trimmed mean Geometric mean, Harmonic mean, Winsorized mean Dispersion Range, Variance, Standard Deviation Interquartile Range (IQR), MAD Coefficient of Variation, Robust scale estimators Position Min, Max, Percentiles Quartiles, Deciles Quantile functions, Order statistics Shape Skewness (concept) Kurtosis (concept) Moment-based shape analysis, Jarque-Bera test Distribution Visualization Histogram, Bar chart Box plot, Violin plot ECDF, Q-Q plot, Ridge plots, Heatmaps Frequency Tables Count, Simple frequency Relative frequency, Cumulative frequency Cross-tabulation, Contingency tables Data Types Numerical vs Categorical Discrete vs Continuous, Ordinal vs Nominal Interval vs Ratio, Mixed-type handling Outlier Detection Visual inspection IQR method (1.5\u00d7 rule) Z-score method, Modified Z-score, Isolation Forest Missing Data Count missing values Missing percentage, Patterns MCAR/MAR/MNAR classification, Missingness heatmap Data Quality Duplicate detection Unique value counts Data validation rules, Schema enforcement Aggregation Sum, Count, Mean GroupBy aggregations Multi-level aggregation, Custom agg functions Bivariate Analysis Scatter plot, Side-by-side bars Covariance, Correlation matrix Pairwise relationships, Conditional distributions Multivariate Analysis \u2014 Pair plots, Correlation heatmaps PCA for exploration, Parallel coordinates Time-Based Descriptives Time period counts Trend lines, Moving averages Seasonal decomposition, Lag features Categorical Summaries Frequency, Mode Proportions, Percentages Entropy, Diversity indices, Mode stability Numerical Summaries 5-number summary Describe() output Extended summaries, Custom summary stats Data Transformation Log, Square root Standardization (Z-score), Normalization (Min-Max) Box-Cox, Yeo-Johnson, Rank transformation Binning Equal-width bins Equal-frequency bins Optimal binning (Sturges, Freedman-Diaconis) Summary Tables Basic stat tables Grouped summary tables Multi-index summary, Styled reports Reporting Basic statistics output Formatted tables (rounding) Automated reports, APA-style tables, Dashboards"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/#quick-reference-descriptive-stats-by-data-type","title":"Quick Reference: Descriptive Stats by Data Type","text":"Data Type Central Tendency Dispersion Visualization Numerical (Normal) Mean Std Dev, Variance Histogram, Box plot Numerical (Skewed) Median IQR, MAD Box plot, Violin plot Ordinal Median, Mode Range, Percentiles Bar chart, Cumulative freq Nominal Mode Frequency, Proportion Bar chart, Pie chart Time Series Moving average Rolling std Line plot, Area chart"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/#essential-python-libraries","title":"Essential Python Libraries","text":"Library Purpose Key Functions <code>pandas</code> Data summarization <code>describe()</code>, <code>agg()</code>, <code>groupby()</code>, <code>value_counts()</code> <code>numpy</code> Numerical operations <code>mean()</code>, <code>std()</code>, <code>percentile()</code>, <code>histogram()</code> <code>scipy.stats</code> Distribution stats <code>skew()</code>, <code>kurtosis()</code>, <code>describe()</code> <code>matplotlib</code> Basic visualization <code>hist()</code>, <code>boxplot()</code>, <code>scatter()</code> <code>seaborn</code> Statistical visualization <code>distplot()</code>, <code>boxplot()</code>, <code>pairplot()</code>, <code>heatmap()</code> <code>pingouin</code> Extended descriptives <code>describe()</code>, <code>summary_stats()</code>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/#key-formulas-reference","title":"Key Formulas Reference","text":"Measure Formula Python Equivalent Mean \u03a3x / n <code>np.mean()</code> Median Middle value <code>np.median()</code> Variance \u03a3(x - mean)\u00b2 / (n-1) <code>np.var(ddof=1)</code> Std Deviation \u221aVariance <code>np.std(ddof=1)</code> IQR Q3 - Q1 <code>np.percentile(75) - np.percentile(25)</code> Skewness E[(x-\u03bc)\u00b3] / \u03c3\u00b3 <code>scipy.stats.skew()</code> Kurtosis E[(x-\u03bc)\u2074] / \u03c3\u2074 - 3 <code>scipy.stats.kurtosis()</code> CV (Std Dev / Mean) \u00d7 100 <code>(std/mean)*100</code> Z-Score (x - mean) / std <code>scipy.stats.zscore()</code>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/#5-number-summary-checklist","title":"5-Number Summary Checklist","text":"Statistic Description Python Minimum Smallest value <code>df.min()</code> Q1 (25th) Lower quartile <code>df.quantile(0.25)</code> Median (50th) Middle value <code>df.median()</code> Q3 (75th) Upper quartile <code>df.quantile(0.75)</code> Maximum Largest value <code>df.max()</code>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/01-central-tendency/","title":"Central Tendency","text":"<p>Central tendency measures describe the \"center\" or \"typical\" value of a dataset. Understanding these is the fundamental first step in any exploratory data analysis.</p>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/01-central-tendency/#beginner-foundations","title":"\ud83d\udfe2 Beginner (Foundations)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/01-central-tendency/#1-mean-arithmetic-average","title":"1. Mean (Arithmetic Average)","text":"<p>The mean is the sum of all values divided by the number of values. It is highly sensitive to outliers.</p> <ul> <li>Formula: \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i\\)</li> <li>Python Implementation:</li> </ul> <pre><code>import pandas as pd\nimport numpy as np\n\ndata = [10, 20, 30, 40, 50, 1000] # 1000 is an outlier\nmean_val = np.mean(data)\nprint(f\"Mean: {mean_val}\") # Output: 191.66\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/01-central-tendency/#2-median","title":"2. Median","text":"<p>The median is the middle value when the data is sorted. It is robust (not heavily affected by outliers).</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>median_val = np.np.median(data)\nprint(f\"Median: {median_val}\") # Output: 35.0\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/01-central-tendency/#3-mode","title":"3. Mode","text":"<p>The mode is the most frequently occurring value. Useful specifically for categorical data.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>from scipy import stats\nmode_val = stats.mode([1, 2, 2, 3, 4], keepdims=True)\nprint(f\"Mode: {mode_val.mode[0]}\") # Output: 2\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/01-central-tendency/#intermediate-engineering","title":"\ud83d\udfe1 Intermediate (Engineering)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/01-central-tendency/#1-weighted-mean","title":"1. Weighted Mean","text":"<p>Used when certain data points contribute more \"weight\" or importance than others (e.g., calculating GPA from credits).</p> <ul> <li>Formula: \\(\\bar{x}_w = \\frac{\\sum_{i=1}^{n}w_i x_i}{\\sum_{i=1}^{n}w_i}\\)</li> <li>Python Implementation:</li> </ul> <pre><code>values = [80, 90, 95]\nweights = [0.2, 0.3, 0.5] # Exams weights\nweighted_mean = np.average(values, weights=weights)\nprint(f\"Weighted Mean: {weighted_mean}\") # Output: 90.5\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/01-central-tendency/#2-trimmed-mean","title":"2. Trimmed Mean","text":"<p>A compromise between the mean and median. It involves discarding a certain percentage of the lowest and highest values before calculating the mean, explicitly removing outliers.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>from scipy.stats import trim_mean\n# Trim 10% from both ends\ntrimmed = trim_mean([1, 2, 3, 4, 5, 6, 7, 8, 9, 1000], proportiontocut=0.1)\nprint(f\"Trimmed Mean: {trimmed}\") # Output: 5.5\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/01-central-tendency/#advanced-elite-optimization","title":"\ud83d\udd34 Advanced / Elite (Optimization)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/01-central-tendency/#1-geometric-mean","title":"1. Geometric Mean","text":"<p>Used for data that grows multiplicatively (e.g., compounding interest rates, investment returns, or biological cell growth). It answers: \"What is the constant growth rate?\"</p> <ul> <li>Formula: \\(GM = \\sqrt[n]{x_1 \\cdot x_2 \\cdots x_n}\\)</li> <li>Python Implementation:</li> </ul> <pre><code>from scipy.stats import gmean\nreturns = [1.05, 1.10, 0.95] # 5% gain, 10% gain, 5% loss\ngeom_mean = gmean(returns)\nprint(f\"Geometric Mean: {geom_mean}\")\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/01-central-tendency/#2-harmonic-mean","title":"2. Harmonic Mean","text":"<p>Used when calculating the average of rates or ratios (e.g., average speed given distance, or P/E ratios in finance).</p> <ul> <li>Formula: \\(HM = \\frac{n}{\\sum_{i=1}^{n}\\frac{1}{x_i}}\\)</li> <li>Python Implementation:</li> </ul> <pre><code>from scipy.stats import hmean\nspeeds = [60, 40] # Outward at 60mph, return at 40mph over same distance\nhar_mean = hmean(speeds)\nprint(f\"Harmonic Mean (Average Speed): {har_mean}\") # Output: 48.0\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/01-central-tendency/#3-winsorized-mean","title":"3. Winsorized Mean","text":"<p>Instead of trimming (removing) outliers, Winsorizing replaces extreme values with the nearest \"acceptable\" value (e.g., capping the 99th percentile). This retains the data point count but reduces variance explosion.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>from scipy.stats import mstats\n# Cap bottom 5% and top 5%\ndata = [1, 2, 3, 4, 5, 6, 7, 8, 9, 100]\nwinsorized_data = mstats.winsorize(data, limits=[0.05, 0.05])\nprint(f\"Winsorized Mean: {np.mean(winsorized_data)}\")\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/","title":"Dispersion and Shape","text":"<p>While Central Tendency tells us where the data \"lives\", Dispersion tells us how \"spread out\" it is. Shape tells us if the data leans heavily to one side (Skewness) or has heavy tails (Kurtosis).</p>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/#beginner-foundations","title":"\ud83d\udfe2 Beginner (Foundations)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/#1-range","title":"1. Range","text":"<p>The absolute difference between the largest and smallest values. It is completely susceptible to outliers.</p> <ul> <li>Formula: \\(Range = Max(x) - Min(x)\\)</li> <li>Python Implementation:</li> </ul> <pre><code>import numpy as np\ndata = [10, 20, 30, 40, 50]\nrange_val = np.ptp(data) # ptp stands for \"peak to peak\"\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/#2-variance-standard-deviation","title":"2. Variance &amp; Standard Deviation","text":"<p>Variance measures the average squared distance from the mean. Standard Deviation (\\(\\sigma\\)) takes the square root of Variance to return the metric back to its original units (e.g., dollars\u00b2 back to dollars).</p> <ul> <li>Formula (Sample Variance): \\(s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n - 1}\\)</li> <li>Python Implementation:</li> </ul> <pre><code>variance = np.var(data, ddof=1) # ddof=1 makes it a Sample Variance\nstd_dev = np.std(data, ddof=1)\nprint(f\"Variance: {variance}, Standard Deviation: {std_dev}\")\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/#3-skewness-concept","title":"3. Skewness (Concept)","text":"<p>Skewness measures the asymmetry of a distribution around its mean.</p> <ul> <li>Positive Skew (Right-skewed): Tail is on the right side. Mean &gt; Median &gt; Mode. (e.g., Household income)</li> <li>Negative Skew (Left-skewed): Tail is on the left side. Mode &gt; Median &gt; Mean. (e.g., Age of retirement)</li> </ul>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/#intermediate-engineering","title":"\ud83d\udfe1 Intermediate (Engineering)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/#1-interquartile-range-iqr","title":"1. Interquartile Range (IQR)","text":"<p>A robust measure of spread. It calculates the range of the middle 50% of the data, completely ignoring the top 25% and bottom 25% outliers.</p> <ul> <li>Formula: \\(IQR = Q_3 - Q_1\\) (75th percentile minus 25th percentile)</li> <li>Python Implementation:</li> </ul> <pre><code>q3, q1 = np.percentile(data, [75, 25])\niqr = q3 - q1\nprint(f\"IQR: {iqr}\")\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/#2-mean-absolute-deviation-mad","title":"2. Mean Absolute Deviation (MAD)","text":"<p>Instead of squaring distances like Variance does (which penalizes extreme outliers heavily), MAD simply takes the absolute value of the distances from the median. Highly robust.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>median = np.median(data)\nmad = np.median(np.abs(data - median))\nprint(f\"Robust MAD: {mad}\")\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/#3-kurtosis-concept","title":"3. Kurtosis (Concept)","text":"<p>Kurtosis measures the \"tailedness\" of the data distribution compared to a Normal Distribution. High kurtosis indicates heavy tails (high likelihood of extreme outliers).</p> <ul> <li>Mesokurtic: Normal distribution behavior (Kurtosis ~3, Excess Kurtosis ~0)</li> <li>Leptokurtic: Heavy tails, sharp peak (Excess Kurtosis &gt; 0)</li> <li>Platykurtic: Light tails, flat peak (Excess Kurtosis &lt; 0)</li> </ul>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/#advanced-elite-optimization","title":"\ud83d\udd34 Advanced / Elite (Optimization)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/#1-coefficient-of-variation-cv","title":"1. Coefficient of Variation (CV)","text":"<p>The ratio of the standard deviation to the mean. It allows you to objectively compare the \"risk\" or volatility of two completely different datasets (like comparing the volatility of a $10 stock vs. a $1000 stock).</p> <ul> <li>Formula: \\(CV = (\\frac{\\sigma}{\\mu}) \\times 100\\)</li> <li>Python Implementation:</li> </ul> <pre><code>mean_val = np.mean(data)\nstd_val = np.std(data, ddof=1)\ncv = (std_val / mean_val) * 100\nprint(f\"Coefficient of Variation: {cv}%\")\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/#2-robust-scale-estimators-rousseeuws-sn-and-qn","title":"2. Robust Scale Estimators (Rousseeuw's Sn and Qn)","text":"<p>In quantitative finance and elite data science, standard deviation breaks instantly. IQR is better but inefficient. <code>Sn</code> and <code>Qn</code> are highly efficient, highly robust estimators of scale that survive up to a 50% contamination rate in your dataset.</p> <ul> <li>Python Implementation (using <code>statsmodels</code>):</li> </ul> <pre><code>import statsmodels.robust.scale as robust\nrc = robust.mad(data) # Normalized MAD\nprint(f\"Robust Scale: {rc}\")\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/#3-moment-based-shape-analysis-moments-1-4","title":"3. Moment-Based Shape Analysis (Moments 1-4)","text":"<p>In probability theory:</p> <ul> <li>1st Moment (Mean): Location</li> <li>2nd Central Moment (Variance): Scale</li> <li>3rd Standardized Moment: Skewness</li> <li>4th Standardized Moment: Kurtosis</li> </ul> <pre><code>from scipy.stats import moment, skew, kurtosis\nm1 = moment(data, moment=1)\nm2 = moment(data, moment=2)\n# Fisher defined normal kurtosis as 0 (Excess Kurtosis)\nkurt = kurtosis(data, fisher=True)\nprint(f\"Skew: {skew(data)}, Excess Kurt: {kurt}\")\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/03-visualization-and-frequency/","title":"Visualization and Frequency","text":"<p>Exploratory Data Analysis (EDA) relies on human visual perception to instantly detect shapes, modalities, and relationships that raw numbers obscure. Frequency tables structure those visual bins locally.</p>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/03-visualization-and-frequency/#beginner-foundations","title":"\ud83d\udfe2 Beginner (Foundations)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/03-visualization-and-frequency/#1-histograms-vs-bar-charts","title":"1. Histograms vs. Bar Charts","text":"<ul> <li>Histograms: Used for Numerical/Continuous quantitative data. They bin raw numbers together into ranges (e.g., Age 10-20, 20-30). Bars touch strictly because data is continuous.</li> <li>Bar Charts: Used for Categorical/Discrete qualitative data. Bars do not touch because categories are discrete (e.g., Red, Blue, Green).</li> </ul> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndata = [12, 15, 14, 22, 28, 30, 31]\nsns.histplot(data, bins=3)\nplt.title(\"Continuous Data Binned\")\nplt.show()\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/03-visualization-and-frequency/#2-count-simple-frequency-tables","title":"2. Count &amp; Simple Frequency Tables","text":"<p>Finding how many times distinct values occur.</p> <pre><code>import pandas as pd\ndf = pd.DataFrame({\"color\": [\"red\", \"blue\", \"red\", \"green\", \"blue\", \"blue\"]})\nprint(df[\"color\"].value_counts()) # Returns raw counts\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/03-visualization-and-frequency/#intermediate-engineering","title":"\ud83d\udfe1 Intermediate (Engineering)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/03-visualization-and-frequency/#1-box-plots-and-violin-plots","title":"1. Box Plots and Violin Plots","text":"<ul> <li>Box Plot: A literal, visual manifestation of the 5-number summary (Min, Q1, Median, Q3, Max) and IQR. Dots outside the \"whiskers\" are mathematical outliers explicitly.</li> <li>Violin Plot: Combines a Box Plot with a Kernel Density Estimate (KDE). It solves the \"bimodality\" problem: A box plot can hide if a distribution has two distinct peaks, but a violin plot shows the exact curvature.</li> </ul> <pre><code>sns.violinplot(x=data)\nplt.title(\"Violin Plot displaying KDE Density Curve\")\nplt.show()\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/03-visualization-and-frequency/#2-relative-and-cumulative-frequency","title":"2. Relative and Cumulative Frequency","text":"<ul> <li>Relative Frequency: Turns raw counts into percentages of the whole population. <code>count / total_count</code>.</li> <li>Cumulative Frequency: A running total. Adds percentages sequentially until 100% is reached.</li> </ul> <pre><code>counts = df[\"color\"].value_counts(normalize=True) * 100\ncumulative = counts.cumsum()\nprint(\"Relative:\\n\", counts)\nprint(\"Cumulative:\\n\", cumulative)\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/03-visualization-and-frequency/#advanced-elite-optimization","title":"\ud83d\udd34 Advanced / Elite (Optimization)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/03-visualization-and-frequency/#1-empirical-cumulative-distribution-function-ecdf","title":"1. Empirical Cumulative Distribution Function (ECDF)","text":"<p>Rather than binning data like a histogram does (which forces you to subjectively choose the <code>bins=</code> parameter), an ECDF plots every single exact data point. It represents the proportion of items less than or equal to <code>x</code>. Fast, objective, and immune to binning bias.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>sns.ecdfplot(data)\nplt.title(\"Exact Cumulative Ratios\")\nplt.show()\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/03-visualization-and-frequency/#2-q-q-plots-quantile-quantile","title":"2. Q-Q Plots (Quantile-Quantile)","text":"<p>The absolute best way to check if your data actually follows a Normal Distribution. It compares your data's quantiles against a theoretical perfect normal distribution. If the data points hug the 45-degree diagonal red line, your data is perfectly normal.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>import scipy.stats as stats\nstats.probplot(data, dist=\"norm\", plot=plt)\nplt.title(\"Q-Q Plot\")\nplt.show()\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/03-visualization-and-frequency/#3-cross-tabulation-contingency-tables","title":"3. Cross-Tabulation &amp; Contingency Tables","text":"<p>Used to quantify the interaction rate between two entirely different categorical variables simultaneously.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>df[\"size\"] = [\"S\", \"M\", \"L\", \"S\", \"M\", \"L\"] # Merging with colors\ncontingency = pd.crosstab(df[\"color\"], df[\"size\"], normalize='index')\nprint(contingency)\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/04-data-quality-and-outliers/","title":"Data Quality and Outliers","text":"<p>Before any machine learning model is trained, the data must be sanitized. Statistical tests must be deployed to handle null-values (missingness) and detect anomalies.</p>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/04-data-quality-and-outliers/#beginner-foundations","title":"\ud83d\udfe2 Beginner (Foundations)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/04-data-quality-and-outliers/#1-visual-inspection-simple-counts","title":"1. Visual Inspection &amp; Simple Counts","text":"<p>Before running algorithms, look at the data structure. Find missing (<code>NaN</code>) values and simple duplicates.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\"Age\": [25, np.nan, 22, 25, 29, 300]}) # Exteme outlier and duplicate\n\nprint(\"Missing Values:\\n\", df.isnull().sum())\nprint(\"Duplicates:\\n\", df.duplicated().sum())\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/04-data-quality-and-outliers/#2-basic-deduplication","title":"2. Basic Deduplication","text":"<p>Removing exact identical rows safely.</p> <pre><code>df_cleaned = df.drop_duplicates()\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/04-data-quality-and-outliers/#intermediate-engineering","title":"\ud83d\udfe1 Intermediate (Engineering)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/04-data-quality-and-outliers/#1-missingness-percentage-matrix","title":"1. Missingness Percentage &amp; Matrix","text":"<p>Instead of raw counts (which are useless if you don't know the dataset size), calculate the percentage of missing values.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>missing_percent = (df.isnull().sum() / len(df)) * 100\nprint(missing_percent, \"% missing\")\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/04-data-quality-and-outliers/#2-iqr-outlier-method-tukeys-15-rule","title":"2. IQR Outlier Method (Tukey's 1.5\u00d7 Rule)","text":"<p>Any data point outside the boundary of \\((Q1 - 1.5 \\times IQR)\\) to \\((Q3 + 1.5 \\times IQR)\\) is mathematically flagged as an outlier. Very robust for skewed data.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>q1 = df[\"Age\"].quantile(0.25)\nq3 = df[\"Age\"].quantile(0.75)\niqr = q3 - q1\n\nlower_bound = q1 - 1.5 * iqr\nupper_bound = q3 + 1.5 * iqr\n\noutliers = df[(df[\"Age\"] &lt; lower_bound) | (df[\"Age\"] &gt; upper_bound)]\nprint(\"Flagged Outliers:\\n\", outliers)\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/04-data-quality-and-outliers/#advanced-elite-optimization","title":"\ud83d\udd34 Advanced / Elite (Optimization)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/04-data-quality-and-outliers/#1-the-z-score-outlier-method","title":"1. The Z-Score Outlier Method","text":"<p>Mathematically calculates how many standard deviations away a data point is from the mean. Typically, any absolute Z-score \\(&gt; 3\\) is considered an anomaly. Note: Z-scores require your data to be Normally Distributed and are highly sensitive to extreme outliers because the mean is used in the calculation.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>from scipy import stats\ndf_nomissing = df.dropna()\nz_scores = np.abs(stats.zscore(df_nomissing[\"Age\"]))\noutliers_z = df_nomissing[z_scores &gt; 3]\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/04-data-quality-and-outliers/#2-statistical-missingness-classification-mcar-mar-mnar","title":"2. Statistical Missingness Classification (MCAR, MAR, MNAR)","text":"<p>You cannot simply \"impute\" missing data with the mean blindly. You must scientifically classify why it is missing.</p> <ul> <li>Missing Completely at Random (MCAR): A true random glitch. Coin flip. (Safe to mean-impute or drop).</li> <li>Missing at Random (MAR): Missingness relies on a different observed variable. (e.g., Men are less likely to fill out depression surveys than women. We can predict missingness using the \"Gender\" column). (Use Multiple Imputation, MICE).</li> <li>Missing Not at Random (MNAR): Missingness relies on the unobserved variable itself. (e.g., People with the lowest incomes leave the \"Income\" column blank because they are embarrassed). (Highly dangerous. Requires specialized domain knowledge).</li> </ul>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/04-data-quality-and-outliers/#3-isolation-forest-unsupervised-anomaly-detection","title":"3. Isolation Forest (Unsupervised Anomaly Detection)","text":"<p>Algorithms like Isolation Forests explicitly isolate anomalies by randomly partitioning trees. Anomalies travel shorter paths than standard observations. Very powerful for multivariable datasets.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>from sklearn.ensemble import IsolationForest\n\nmodel = IsolationForest(contamination=0.05)\npredictions = model.fit_predict(df_nomissing[[\"Age\"]])\n# -1 indicates an outlier anomaly\ndf_nomissing[\"Anomaly\"] = predictions\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/05-bivariate-and-multivariate/","title":"Bivariate and Multivariate","text":"<p>Moving beyond single variables (univariate) to explore how two or more variables interact with one another simultaneously.</p>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/05-bivariate-and-multivariate/#beginner-foundations","title":"\ud83d\udfe2 Beginner (Foundations)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/05-bivariate-and-multivariate/#1-scatter-plots","title":"1. Scatter Plots","text":"<p>The fundamental plotting technique to see the raw visual relationship between two numerical variables.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = sns.load_dataset(\"tips\")\n\nsns.scatterplot(x=\"total_bill\", y=\"tip\", data=df)\nplt.title(\"Total Bill vs. Tip Amount\")\nplt.show()\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/05-bivariate-and-multivariate/#2-side-by-side-bar-charts","title":"2. Side-by-Side Bar Charts","text":"<p>Comparing continuous outputs across multiple different categories visually.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>sns.barplot(x=\"day\", y=\"total_bill\", hue=\"smoker\", data=df)\nplt.title(\"Bill by Day and Smoker Status\")\nplt.show()\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/05-bivariate-and-multivariate/#intermediate-engineering","title":"\ud83d\udfe1 Intermediate (Engineering)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/05-bivariate-and-multivariate/#1-covariance-vs-correlation","title":"1. Covariance vs. Correlation","text":"<ul> <li>Covariance: Measures the direction of the relationship between two variables. Very difficult to interpret because the scale relies entirely on the original data units (e.g., measuring height in miles vs. inches changes covariance completely).</li> <li>Correlation (Pearson's \\(r\\)): The standardized version of covariance. It objectively bounds the relationship strictly between <code>-1</code> (perfect inverse) and <code>+1</code> (perfect positive). A value of <code>0</code> means absolutely zero linear relationship.</li> </ul> <pre><code>correlation = df[[\"total_bill\", \"tip\"]].corr()\nprint(\"Pearson Correlation:\\n\", correlation)\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/05-bivariate-and-multivariate/#2-correlation-matrices-and-heatmaps","title":"2. Correlation Matrices and Heatmaps","text":"<p>Applying the underlying \\(r\\)-correlation formula to every single numerical column against every other numerical column simultaneously to build a matrix.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>numeric_df = df.select_dtypes(include=['float64', 'int64'])\nmatrix = numeric_df.corr()\n\nsns.heatmap(matrix, annot=True, cmap=\"coolwarm\")\nplt.title(\"Correlation Heatmap\")\nplt.show()\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/05-bivariate-and-multivariate/#3-pair-plots","title":"3. Pair Plots","text":"<p>Creates a grid of scatterplots for every single pair of variables while drawing the histogram distributions diagonally.</p> <pre><code>sns.pairplot(df, hue=\"sex\")\nplt.show()\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/05-bivariate-and-multivariate/#advanced-elite-optimization","title":"\ud83d\udd34 Advanced / Elite (Optimization)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/05-bivariate-and-multivariate/#1-pairwise-non-linear-relationships-spearman-kendall","title":"1. Pairwise Non-Linear Relationships (Spearman &amp; Kendall)","text":"<p>Pearson only detects linear relationships (straight lines). If <code>y = x^2</code> (an exponential curve), Pearson might read <code>0.0</code>.</p> <ul> <li>Spearman's Rho: Looks strictly at the rank order of variables. If variable X goes up, does variable Y go up (regardless of how fast)?</li> <li>Kendall's Tau: Excellent for incredibly small sample sizes where ranking is critical.</li> </ul> <pre><code>spearman_corr = df[[\"total_bill\", \"tip\"]].corr(method=\"spearman\")\nprint(\"Spearman Rank Correlation:\\n\", spearman_corr)\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/05-bivariate-and-multivariate/#2-pca-principal-component-analysis-for-eda-exploration","title":"2. PCA (Principal Component Analysis) for EDA Exploration","text":"<p>When a dataset has 50 columns, you cannot possibly visualize 50 dimensions on a flat screen or find simple pair plots. PCA is an extremely powerful linear algebra technique (SVD) applied to rotationally project 50 dimensions flat onto a 2D or 3D graph (the \"Components\" capturing the maximum variance).</p> <pre><code>from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Standardize data strictly first before PCA\nscaler = StandardScaler()\nscaled = scaler.fit_transform(numeric_df)\n\npca = PCA(n_components=2)\nprincipal_components = pca.fit_transform(scaled)\n\nplt.scatter(principal_components[:, 0], principal_components[:, 1])\nplt.xlabel(\"First Principal Component\")\nplt.ylabel(\"Second Principal Component\")\nplt.title(\"2D Projection of High-Dimensional Variance\")\nplt.show()\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/","title":"Advanced Summaries","text":"<p>Transforming data shapes into workable ML structures, generating grouped aggregations, and standardizing values for modeling calculations.</p>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#beginner-foundations","title":"\ud83d\udfe2 Beginner (Foundations)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#1-simple-data-transformations-log-square-root","title":"1. Simple Data Transformations (Log, Square Root)","text":"<p>Highly skewed data breaks linear assumptions in machine learning. We use static mathematical transformations to compress those massive outliers and force the distribution closer to a Bell Curve/Normal structure.</p> <ul> <li>Log Transform: \\(y = \\log(x+1)\\) (The \\(+1\\) handles datasets with exact zeros).</li> <li>Square Root Transform: \\(y = \\sqrt{x}\\)</li> </ul> <pre><code>import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\ndf = sns.load_dataset(\"diamonds\")\n\n# Log transform the heavily right-skewed price column\ndf[\"log_price\"] = np.log1p(df[\"price\"])\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#2-basic-grouping-and-tables","title":"2. Basic Grouping and Tables","text":"<p>Aggregating numbers by discrete categories.</p> <pre><code>mean_price_by_cut = df.groupby(\"cut\")[\"price\"].mean()\nprint(mean_price_by_cut)\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#3-equal-width-binning","title":"3. Equal-width Binning","text":"<p>Slicing continuous numerical data into discrete categories by explicitly typing ranges (e.g. \\([0 \\to 10]\\), \\([10 \\to 20]\\)).</p> <pre><code>df[\"price_category\"] = pd.cut(df[\"price\"], bins=3, labels=[\"Low\", \"Med\", \"High\"])\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#intermediate-engineering","title":"\ud83d\udfe1 Intermediate (Engineering)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#1-standardization-z-score","title":"1. Standardization (Z-score)","text":"<p>Transforms continuous data so its <code>Mean = 0</code> and its <code>Standard Deviation = 1</code>. Vital for neural networks, SVMs, Ridge regression, and PCA visualization where large numbers mathematically overpower small numbers.</p> <ul> <li>Formula: \\(Z = \\frac{x - \\mu}{\\sigma}\\)</li> </ul> <pre><code>from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndf[\"scaled_price\"] = scaler.fit_transform(df[[\"price\"]])\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#2-normalization-min-max","title":"2. Normalization (Min-Max)","text":"<p>Forcibly compresses the entire dataset perfectly between <code>0</code> and <code>1</code>. Highly sensitive to extreme outliers because the <code>Max</code> anchors all other values. Used heavily for Image/Pixel data in ConvNets.</p> <ul> <li>Formula: \\(X_{norm} = \\frac{x - x_{min}}{x_{max} - x_{min}}\\)</li> </ul>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#3-equal-frequency-binning-quantile-binning","title":"3. Equal-frequency Binning (Quantile Binning)","text":"<p>Unlike equal-width binning, this ensures every single bin contains the exact same number of samples, avoiding massive class imbalances.</p> <pre><code># 4 bins (quartiles), each receiving 25% of the data exactly\ndf[\"price_quantile\"] = pd.qcut(df[\"price\"], q=4, labels=[\"Q1\", \"Q2\", \"Q3\", \"Q4\"])\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#4-groupby-aggregations-with-multiple-functions","title":"4. GroupBy Aggregations with Multiple Functions","text":"<pre><code>summary = df.groupby(\"cut\").agg({\n    \"price\": [\"mean\", \"median\", \"std\"],\n    \"carat\": [\"max\"]\n})\nprint(summary)\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#advanced-elite-optimization","title":"\ud83d\udd34 Advanced / Elite (Optimization)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#1-optimal-binning-freedman-diaconis-rule","title":"1. Optimal Binning (Freedman-Diaconis Rule)","text":"<p>Instead of guessing <code>bins=30</code> subjectively in a histogram, mathematically optimize the bin width to perfectly represent the underlying density, adjusting rigidly for sample size and IQR to ignore extreme outliers plotting themselves.</p> <ul> <li>Formula: \\(Bin\\_Width = 2 \\times \\frac{IQR(x)}{\\sqrt[3]{n}}\\)</li> </ul> <pre><code>import matplotlib.pyplot as plt\n\n# Numpy's 'fd' string natively applies Freedman-Diaconis equation\nplt.hist(df[\"price\"], bins=\"fd\")\nplt.title(\"Freedman-Diaconis Optimized Bins\")\nplt.show()\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#2-power-transformations-box-cox-yeo-johnson","title":"2. Power Transformations (Box-Cox &amp; Yeo-Johnson)","text":"<p>Instead of \"guessing\" whether to use Log or Square Root, these are advanced algorithms that rigorously hunt for the exact mathematical \\(\\lambda\\) exponent that transforms your specific arbitrary data point structure into a perfect Gaussian Normal Distribution.</p> <ul> <li>Box-Cox: Data strictly must be positive ( \\(&gt; 0\\)).</li> <li>Yeo-Johnson: Elegantly handles negative numbers and exact zeros implicitly.</li> </ul> <pre><code>from sklearn.preprocessing import PowerTransformer\n\n# Defaults to Yeo-Johnson\npt = PowerTransformer()\ndf[\"gaussian_price\"] = pt.fit_transform(df[[\"price\"]])\nprint(f\"Algorithm applied Lambda: {pt.lambdas_[0]}\")\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#3-rank-transformation-non-parametric","title":"3. Rank Transformation (Non-Parametric)","text":"<p>For situations absolutely dominated by savage outliers where nothing fixes the skew, you abandon the values completely and sort the list. The smallest number becomes <code>1</code>, the second becomes <code>2</code>, the billion outlier becomes <code>3</code>. This permanently flattens the distribution into a uniform rectangle, erasing distances but guaranteeing model convergence stability.</p> <pre><code>df[\"rank_price\"] = df[\"price\"].rank()\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/distributions/","title":"DISTRIBUTIONS","text":""},{"location":"month-01/week-02-statistics-probability/hypothesis-testing/","title":"HYPOTHESIS TESTING","text":""},{"location":"month-01/week-02-statistics-probability/probability/","title":"PROBABILITY","text":""},{"location":"month-01/week-03-core-ml-algorithms/decision-trees/","title":"DECISION TREES","text":""},{"location":"month-01/week-03-core-ml-algorithms/linear-regression/","title":"LINEAR REGRESSION","text":""},{"location":"month-01/week-03-core-ml-algorithms/logistic-regression/","title":"LOGISTIC REGRESSION","text":""},{"location":"month-01/week-04-pytorch-scikit-mlflow/mlflow-tracking/","title":"MLFLOW TRACKING","text":""},{"location":"month-01/week-04-pytorch-scikit-mlflow/pytorch-basics/","title":"PYTORCH BASICS","text":""},{"location":"month-01/week-04-pytorch-scikit-mlflow/scikit-learn/","title":"SCIKIT LEARN","text":""},{"location":"month-02/","title":"Month 2","text":""},{"location":"month-02/week-05-deep-learning-cnns-rnns/cnns/","title":"CNNS","text":""},{"location":"month-02/week-05-deep-learning-cnns-rnns/rnns/","title":"RNNS","text":""},{"location":"month-02/week-05-deep-learning-cnns-rnns/transformers-intro/","title":"TRANSFORMERS INTRO","text":""},{"location":"month-02/week-06-nlp-bert-huggingface/bert-finetuning/","title":"BERT FINETUNING","text":""},{"location":"month-02/week-06-nlp-bert-huggingface/huggingface-ecosystem/","title":"HUGGINGFACE ECOSYSTEM","text":""},{"location":"month-02/week-06-nlp-bert-huggingface/tokenization/","title":"TOKENIZATION","text":""},{"location":"month-02/week-07-llms-rag-vectordbs/llm-internals/","title":"LLM INTERNALS","text":""},{"location":"month-02/week-07-llms-rag-vectordbs/rag-pipelines/","title":"RAG PIPELINES","text":""},{"location":"month-02/week-07-llms-rag-vectordbs/vector-databases/","title":"VECTOR DATABASES","text":""},{"location":"month-02/week-08-llm-finetuning-eval/evaluation-ragas/","title":"EVALUATION RAGAS","text":""},{"location":"month-02/week-08-llm-finetuning-eval/lora-qlora/","title":"LORA QLORA","text":""},{"location":"month-02/week-08-llm-finetuning-eval/unsloth/","title":"UNSLOTH","text":""},{"location":"month-03/","title":"Month 3","text":""},{"location":"month-03/week-09-fastapi-docker-rest/docker-containers/","title":"DOCKER CONTAINERS","text":""},{"location":"month-03/week-09-fastapi-docker-rest/fastapi/","title":"FASTAPI","text":""},{"location":"month-03/week-09-fastapi-docker-rest/rest-api-design/","title":"REST API DESIGN","text":""},{"location":"month-03/week-10-mlops-mlflow-dvc-cicd/dvc-data-versioning/","title":"DVC DATA VERSIONING","text":""},{"location":"month-03/week-10-mlops-mlflow-dvc-cicd/github-actions-cicd/","title":"GITHUB ACTIONS CICD","text":""},{"location":"month-03/week-10-mlops-mlflow-dvc-cicd/mlops-pipelines/","title":"MLOPS PIPELINES","text":""},{"location":"month-03/week-11-cloud-aws-sagemaker/aws-fundamentals/","title":"AWS FUNDAMENTALS","text":""},{"location":"month-03/week-11-cloud-aws-sagemaker/s3-ec2/","title":"S3 EC2","text":""},{"location":"month-03/week-11-cloud-aws-sagemaker/sagemaker/","title":"SAGEMAKER","text":""},{"location":"month-03/week-12-kubernetes-monitoring/grafana/","title":"GRAFANA","text":""},{"location":"month-03/week-12-kubernetes-monitoring/kubernetes/","title":"KUBERNETES","text":""},{"location":"month-03/week-12-kubernetes-monitoring/prometheus/","title":"PROMETHEUS","text":""},{"location":"month-03/week-13-aws-cert-prep/cloud-practitioner/","title":"CLOUD PRACTITIONER","text":""},{"location":"month-03/week-13-aws-cert-prep/solutions-architect/","title":"SOLUTIONS ARCHITECT","text":""},{"location":"month-04/","title":"Month 4","text":""},{"location":"month-04/week-14-langgraph-multi-agent/crewai/","title":"CREWAI","text":""},{"location":"month-04/week-14-langgraph-multi-agent/langchain/","title":"LANGCHAIN","text":""},{"location":"month-04/week-14-langgraph-multi-agent/langgraph/","title":"LANGGRAPH","text":""},{"location":"month-04/week-15-data-pipelines-airflow-dbt/airflow/","title":"AIRFLOW","text":""},{"location":"month-04/week-15-data-pipelines-airflow-dbt/bigquery/","title":"BIGQUERY","text":""},{"location":"month-04/week-15-data-pipelines-airflow-dbt/dbt-transformations/","title":"DBT TRANSFORMATIONS","text":""},{"location":"month-04/week-16-spark-kafka-streaming/apache-spark/","title":"APACHE SPARK","text":""},{"location":"month-04/week-16-spark-kafka-streaming/kafka/","title":"KAFKA","text":""},{"location":"month-04/week-16-spark-kafka-streaming/streaming-basics/","title":"STREAMING BASICS","text":""},{"location":"month-04/week-17-system-design-capstone/capstone-planning/","title":"CAPSTONE PLANNING","text":""},{"location":"month-04/week-17-system-design-capstone/system-design/","title":"SYSTEM DESIGN","text":""},{"location":"month-05/","title":"Month 5","text":""},{"location":"month-05/week-18-ml-interview-theory/core-ml-questions/","title":"CORE ML QUESTIONS","text":""},{"location":"month-05/week-18-ml-interview-theory/theory-deep-dive/","title":"THEORY DEEP DIVE","text":""},{"location":"month-05/week-19-system-design-ml/llm-search-scale/","title":"LLM SEARCH SCALE","text":""},{"location":"month-05/week-19-system-design-ml/real-time-fraud/","title":"REAL TIME FRAUD","text":""},{"location":"month-05/week-20-mock-interviews/behavioral-prep/","title":"BEHAVIORAL PREP","text":""},{"location":"month-05/week-20-mock-interviews/coding-interviews/","title":"CODING INTERVIEWS","text":""},{"location":"month-05/week-21-kaggle-sprint-aws/aws-saa-exam/","title":"AWS SAA EXAM","text":""},{"location":"month-05/week-21-kaggle-sprint-aws/kaggle-competition/","title":"KAGGLE COMPETITION","text":""},{"location":"month-06/","title":"Month 6","text":""},{"location":"month-06/week-22-capstone-core-build/backend-development/","title":"BACKEND DEVELOPMENT","text":""},{"location":"month-06/week-22-capstone-core-build/model-deployment/","title":"MODEL DEPLOYMENT","text":""},{"location":"month-06/week-23-capstone-frontend-demo/live-demo-deployment/","title":"LIVE DEMO DEPLOYMENT","text":""},{"location":"month-06/week-23-capstone-frontend-demo/streamlit-gradio/","title":"STREAMLIT GRADIO","text":""},{"location":"month-06/week-24-tech-blog-arch-walkthrough/architecture-diagrams/","title":"ARCHITECTURE DIAGRAMS","text":""},{"location":"month-06/week-24-tech-blog-arch-walkthrough/blog-drafting/","title":"BLOG DRAFTING","text":""},{"location":"month-06/week-25-portfolio-polish/github-readme/","title":"GITHUB README","text":""},{"location":"month-06/week-25-portfolio-polish/resume-optimization/","title":"RESUME OPTIMIZATION","text":""},{"location":"month-06/week-26-career-launch/gcp-ace-prep/","title":"GCP ACE PREP","text":""},{"location":"month-06/week-26-career-launch/job-applications/","title":"JOB APPLICATIONS","text":""},{"location":"projects/","title":"Projects","text":""},{"location":"resources/","title":"Resources","text":""}]}