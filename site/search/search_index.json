{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Utkarsh AI Labs","text":"<p>A 6-month AI/ML learning academy. Theory, code, proof-of-work commits.</p> Month Focus 1 Mathematical Foundations 2 Classical ML &amp; Feature Engineering 3 Deep Learning &amp; NLP 4 MLOps &amp; Deployment 5 GenAI &amp; LLM Engineering 6 Capstone &amp; Portfolio <p>Current: Month 1 \u2014 Foundations \u00b7 Dashboard</p>"},{"location":"dashboard/","title":"Dashboard","text":"<p>Current: Month 1 \u00b7 Statistics &amp; Probability \u00b7 In Progress</p>"},{"location":"dashboard/#progress","title":"Progress","text":"Module Topic Status M0 Statistics &amp; Probability  Active M1 Linear Algebra \u2014 M2 Calculus &amp; Optimization \u2014 M3 Information Theory \u2014"},{"location":"dashboard/#weekly-log","title":"Weekly Log","text":"Week Summary 1 Hypothesis testing lesson, StatisticalValidator lab, proof-of-work challenge"},{"location":"month-01/","title":"Month 1 \u2014 Foundations","text":"Module Topic Status M0 Statistics &amp; Probability Active M1 Linear Algebra \u2014 M2 Calculus &amp; Optimization \u2014 M3 Information Theory \u2014"},{"location":"month-01/module-00-statistics/","title":"M0 \u2014 Statistics &amp; Probability","text":"<p>The statistical toolkit for model comparison, A/B testing, and uncertainty quantification.</p>"},{"location":"month-01/module-00-statistics/#lessons","title":"Lessons","text":"# Topic 01 Hypothesis Testing in ML Pipelines"},{"location":"month-01/module-00-statistics/#labs","title":"Labs","text":"<ul> <li>Implementation Lab: Statistical Validator</li> <li>Proof of Work</li> </ul>"},{"location":"month-01/module-00-statistics/01_hypothesis_testing/","title":"Statistical Significance &amp; Hypothesis Testing in ML Pipelines","text":"<p>Month 1 \u00b7 Module 0 \u00b7 Lesson 01</p>","tags":["statistics","hypothesis-testing","a-b-testing","month-01"]},{"location":"month-01/module-00-statistics/01_hypothesis_testing/#1-the-problem-this-solves","title":"1. The Problem This Solves","text":"<p>You trained two models. Model B has an accuracy of 91.3%; Model A has 90.8%. Your manager asks:</p> <p>\"Is B actually better, or did we get lucky on the test split?\"</p> <p>If you cannot answer that question with a number and a confidence level, you are not doing science \u2014 you are doing astrology. This lesson gives you the tools to answer rigorously.</p>","tags":["statistics","hypothesis-testing","a-b-testing","month-01"]},{"location":"month-01/module-00-statistics/01_hypothesis_testing/#2-core-concepts-mathematical-intuition","title":"2. Core Concepts \u2014 Mathematical Intuition","text":"","tags":["statistics","hypothesis-testing","a-b-testing","month-01"]},{"location":"month-01/module-00-statistics/01_hypothesis_testing/#21-the-null-hypothesis-h_0","title":"2.1 The Null Hypothesis (\\(H_0\\))","text":"<p>The null hypothesis is the conservative stance: \"There is no real difference between Model A and Model B. Any observed difference is due to random sampling.\"</p> \\[ H_0: \\mu_A = \\mu_B \\quad \\text{(no difference in population means)} \\] \\[ H_1: \\mu_A \\neq \\mu_B \\quad \\text{(there IS a real difference)} \\] <p>Why default to \\(H_0\\)?</p> <p>In production ML, false positives are expensive. Deploying Model B when it isn't actually better means wasted compute, potential regressions, and eroded stakeholder trust. The null hypothesis forces you to prove improvement beyond reasonable doubt.</p>","tags":["statistics","hypothesis-testing","a-b-testing","month-01"]},{"location":"month-01/module-00-statistics/01_hypothesis_testing/#22-the-t-statistic","title":"2.2 The T-Statistic","text":"<p>The T-statistic measures how many standard errors the observed difference is away from zero (the null). The larger the \\(|t|\\), the less plausible \\(H_0\\) becomes.</p> <p>For two independent samples with unequal variances (Welch's T-test):</p> \\[ t = \\frac{\\bar{X}_A - \\bar{X}_B}{\\sqrt{\\frac{s_A^2}{n_A} + \\frac{s_B^2}{n_B}}} \\] <p>Where:</p> Symbol Meaning \\(\\bar{X}_A, \\bar{X}_B\\) Sample means (e.g., mean accuracy across \\(k\\)-fold runs) \\(s_A^2, s_B^2\\) Sample variances \\(n_A, n_B\\) Number of observations (e.g., number of folds) <p>Intuition Check</p> <p>The numerator is the signal (observed difference). The denominator is the noise (pooled uncertainty). A large \\(t\\) means signal &gt;&gt; noise.</p>","tags":["statistics","hypothesis-testing","a-b-testing","month-01"]},{"location":"month-01/module-00-statistics/01_hypothesis_testing/#23-the-p-value","title":"2.3 The P-Value","text":"<p>The P-value is the probability of observing a test statistic at least as extreme as the one computed, assuming \\(H_0\\) is true.</p> \\[ p = P\\!\\left(|T| \\geq |t_{\\text{obs}}| \\;\\middle|\\; H_0\\right) \\] <p>What the P-value is NOT</p> <ul> <li>It is not the probability that \\(H_0\\) is true.</li> <li>It is not the probability that you made an error.</li> <li>It is a measure of surprise: \"If there were truly no difference, how unlikely is this data?\"</li> </ul> <p>Decision rule at significance level \\(\\alpha\\):</p> \\[ \\text{Reject } H_0 \\iff p &lt; \\alpha \\] <p>Convention: \\(\\alpha = 0.05\\) (5% false positive rate). In high-stakes ML (medical, financial), use \\(\\alpha = 0.01\\).</p>","tags":["statistics","hypothesis-testing","a-b-testing","month-01"]},{"location":"month-01/module-00-statistics/01_hypothesis_testing/#24-degrees-of-freedom-welch-satterthwaite","title":"2.4 Degrees of Freedom (Welch-Satterthwaite)","text":"<p>For Welch's T-test, degrees of freedom are approximated:</p> \\[ \\nu \\approx \\frac{\\left(\\frac{s_A^2}{n_A} + \\frac{s_B^2}{n_B}\\right)^2}{\\frac{\\left(\\frac{s_A^2}{n_A}\\right)^2}{n_A - 1} + \\frac{\\left(\\frac{s_B^2}{n_B}\\right)^2}{n_B - 1}} \\] <p>You rarely compute this by hand \u2014 <code>scipy.stats.ttest_ind</code> does it for you \u2014 but understanding it matters when \\(n_A \\neq n_B\\).</p>","tags":["statistics","hypothesis-testing","a-b-testing","month-01"]},{"location":"month-01/module-00-statistics/01_hypothesis_testing/#3-the-so-what-why-data-scientists-need-this","title":"3. The \"So What?\" \u2014 Why Data Scientists Need This","text":"","tags":["statistics","hypothesis-testing","a-b-testing","month-01"]},{"location":"month-01/module-00-statistics/01_hypothesis_testing/#31-ab-testing-in-production","title":"3.1 A/B Testing in Production","text":"<p>Every tech company runs A/B tests: new recommendation model vs. old, new ranking algorithm vs. baseline. Without hypothesis testing:</p> <ul> <li>You deploy based on anecdote (\"the click rate looked higher last Tuesday\").</li> <li>You cannot compute required sample size before launch.</li> <li>You risk peeking at results too early and making premature decisions.</li> </ul> <p>Real-World Scenario</p> <p>Your e-commerce team wants to test a new product ranking model. With hypothesis testing, you can answer:</p> <ol> <li>Before launch: \"We need 10,000 sessions per variant to detect a 2% lift at \\(\\alpha=0.05\\), power \\(= 0.8\\).\" (power analysis)</li> <li>After collection: \"The p-value is 0.003. We reject \\(H_0\\) \u2014 the new model's conversion rate is statistically significantly higher.\"</li> <li>Confidence interval: \"The true lift is between 1.2% and 3.1% (95% CI).\"</li> </ol>","tags":["statistics","hypothesis-testing","a-b-testing","month-01"]},{"location":"month-01/module-00-statistics/01_hypothesis_testing/#32-model-comparison-in-ml-pipelines","title":"3.2 Model Comparison in ML Pipelines","text":"The Wrong WayThe Right Way <pre><code>Model A accuracy: 90.8%\nModel B accuracy: 91.3%\n\u2192 Ship Model B \u2713\n</code></pre> <p>This compares single point estimates with zero uncertainty quantification.</p> <pre><code>Model A (10-fold CV): 90.8% \u00b1 1.2%\nModel B (10-fold CV): 91.3% \u00b1 1.4%\nWelch's t-test p-value: 0.23\n\u2192 FAIL to reject H\u2080. Difference is NOT significant.\n\u2192 Keep Model A (simpler, already deployed).\n</code></pre> <p>This is what a job-ready data scientist does.</p>","tags":["statistics","hypothesis-testing","a-b-testing","month-01"]},{"location":"month-01/module-00-statistics/01_hypothesis_testing/#33-guarding-against-common-pitfalls","title":"3.3 Guarding Against Common Pitfalls","text":"Pitfall What Goes Wrong The Fix Multiple comparisons Testing 20 models \u2192 ~1 will \"win\" by chance at \\(\\alpha = 0.05\\) Bonferroni correction: \\(\\alpha' = \\alpha / m\\) Peeking Checking p-value daily until it's significant Pre-register sample size; use sequential testing Small \\(n\\) 3-fold CV gives you \\(n=3\\) \u2014 huge variance Use 10-fold or repeated stratified k-fold Ignoring effect size \\(p = 0.001\\) but lift is 0.01% \u2014 who cares? Always report Cohen's d alongside \\(p\\)","tags":["statistics","hypothesis-testing","a-b-testing","month-01"]},{"location":"month-01/module-00-statistics/01_hypothesis_testing/#4-implementation-lab-comparing-two-model-versions","title":"4. Implementation Lab \u2014 Comparing Two Model Versions","text":"","tags":["statistics","hypothesis-testing","a-b-testing","month-01"]},{"location":"month-01/module-00-statistics/01_hypothesis_testing/#41-setup","title":"4.1 Setup","text":"<pre><code># requirements: scipy, numpy, scikit-learn\nimport numpy as np\nfrom scipy import stats\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n</code></pre>","tags":["statistics","hypothesis-testing","a-b-testing","month-01"]},{"location":"month-01/module-00-statistics/01_hypothesis_testing/#42-collect-cross-validation-scores","title":"4.2 Collect Cross-Validation Scores","text":"<pre><code># Load dataset\nX, y = load_breast_cancer(return_X_y=True)\n\n# Model A: Logistic Regression (baseline)\nmodel_a = LogisticRegression(max_iter=10_000, random_state=42)\nscores_a = cross_val_score(model_a, X, y, cv=10, scoring=\"accuracy\")\n\n# Model B: Random Forest (challenger)\nmodel_b = RandomForestClassifier(n_estimators=100, random_state=42)\nscores_b = cross_val_score(model_b, X, y, cv=10, scoring=\"accuracy\")\n\nprint(f\"Model A \u2014 Mean: {scores_a.mean():.4f}, Std: {scores_a.std():.4f}\")\nprint(f\"Model B \u2014 Mean: {scores_b.mean():.4f}, Std: {scores_b.std():.4f}\")\n</code></pre>","tags":["statistics","hypothesis-testing","a-b-testing","month-01"]},{"location":"month-01/module-00-statistics/01_hypothesis_testing/#43-welchs-t-test","title":"4.3 Welch's T-Test","text":"<pre><code># Welch's t-test (does NOT assume equal variances) # (1)!\nt_stat, p_value = stats.ttest_ind(scores_a, scores_b, equal_var=False)\n\nalpha = 0.05\n\nprint(f\"\\n--- Hypothesis Test Results ---\")\nprint(f\"T-statistic:  {t_stat:.4f}\")\nprint(f\"P-value:      {p_value:.4f}\")\nprint(f\"Alpha:        {alpha}\")\nprint(f\"Significant:  {'YES \u2014 reject H\u2080' if p_value &lt; alpha else 'NO \u2014 fail to reject H\u2080'}\")\n</code></pre> <ol> <li>Setting <code>equal_var=False</code> switches from Student's t-test to Welch's t-test \u2014 always safer when you can't guarantee both models have the same variance across folds.</li> </ol>","tags":["statistics","hypothesis-testing","a-b-testing","month-01"]},{"location":"month-01/module-00-statistics/01_hypothesis_testing/#44-effect-size-cohens-d","title":"4.4 Effect Size (Cohen's d)","text":"<pre><code>def cohens_d(group1: np.ndarray, group2: np.ndarray) -&gt; float: # (1)!\n    \"\"\"Compute Cohen's d for independent samples.\"\"\"\n    n1, n2 = len(group1), len(group2)\n    pooled_std = np.sqrt(\n        ((n1 - 1) * group1.std(ddof=1)**2 + (n2 - 1) * group2.std(ddof=1)**2)\n        / (n1 + n2 - 2)\n    )\n    return (group1.mean() - group2.mean()) / pooled_std\n\nd = cohens_d(scores_a, scores_b)\nprint(f\"Cohen's d:    {d:.4f}\")\nprint(f\"Effect size:  {'Small' if abs(d) &lt; 0.5 else 'Medium' if abs(d) &lt; 0.8 else 'Large'}\")\n</code></pre> <ol> <li>Cohen's d thresholds: Small (\\(|d| &lt; 0.5\\)), Medium (\\(0.5 \\leq |d| &lt; 0.8\\)), Large (\\(|d| \\geq 0.8\\)).</li> </ol>","tags":["statistics","hypothesis-testing","a-b-testing","month-01"]},{"location":"month-01/module-00-statistics/01_hypothesis_testing/#45-confidence-interval-for-the-difference-in-means","title":"4.5 Confidence Interval for the Difference in Means","text":"<pre><code>diff = scores_a.mean() - scores_b.mean()\nse_diff = np.sqrt(scores_a.var(ddof=1)/len(scores_a) + scores_b.var(ddof=1)/len(scores_b))\n\n# 95% CI using t-distribution\ndf = len(scores_a) + len(scores_b) - 2  # approximate\nt_crit = stats.t.ppf(1 - alpha/2, df)\n\nci_lower = diff - t_crit * se_diff\nci_upper = diff + t_crit * se_diff\n\nprint(f\"\\n95% CI for (\u03bc_A - \u03bc_B): [{ci_lower:.4f}, {ci_upper:.4f}]\")\nprint(\"\u2192 If CI contains 0, the difference is not significant.\")\n</code></pre>","tags":["statistics","hypothesis-testing","a-b-testing","month-01"]},{"location":"month-01/module-00-statistics/01_hypothesis_testing/#5-decision-framework-putting-it-all-together","title":"5. Decision Framework \u2014 Putting It All Together","text":"<pre><code>flowchart TD\n    A[\"Trained Model A &amp; Model B\"] --&gt; B[\"Run k-fold CV\\n(k \u2265 10)\"]\n    B --&gt; C[\"Compute Welch's t-test\"]\n    C --&gt; D{p &lt; \u03b1?}\n    D -- \"Yes\" --&gt; E[\"Report effect size\\n(Cohen's d)\"]\n    E --&gt; F{\"|d| meaningful?\"}\n    F -- \"Yes\" --&gt; G[\"\u2705 Deploy Model B\\n+ log CI\"]\n    F -- \"No\" --&gt; H[\"\u26a0\ufe0f Statistically significant\\nbut practically irrelevant.\\nKeep Model A.\"]\n    D -- \"No\" --&gt; I[\"\u274c Fail to reject H\u2080.\\nKeep Model A.\"]</code></pre> <p>The Golden Rule</p> <p>Statistical significance without practical significance is noise. Always pair the p-value with effect size and a confidence interval. Your stakeholders don't care about p-values \u2014 they care about \"how much better, and how sure are you?\"</p>","tags":["statistics","hypothesis-testing","a-b-testing","month-01"]},{"location":"month-01/module-00-statistics/01_hypothesis_testing/#6-key-takeaways","title":"6. Key Takeaways","text":"Concept One-Liner \\(H_0\\) \"There is no difference\" \u2014 your default until proven otherwise T-statistic Signal-to-noise ratio of the observed difference P-value How surprised you should be if \\(H_0\\) were true \\(\\alpha\\) Your tolerance for false positives (typically 0.05) Cohen's d The magnitude of the difference (small / medium / large) Confidence Interval The plausible range of the true difference","tags":["statistics","hypothesis-testing","a-b-testing","month-01"]},{"location":"month-01/module-00-statistics/01_hypothesis_testing/#next-steps","title":"Next Steps","text":"<p> Implementation Lab: Statistical Validator \u2014 Package this into a reusable class.</p> <p> Proof of Work \u2014 The challenge you must commit to GitHub.</p>","tags":["statistics","hypothesis-testing","a-b-testing","month-01"]},{"location":"month-01/module-00-statistics/lab_statistical_validator/","title":"Implementation Lab: Statistical Validator","text":"<p>Lab \u00b7 Module 0</p>","tags":["lab","statistics","month-01"]},{"location":"month-01/module-00-statistics/lab_statistical_validator/#objective","title":"Objective","text":"<p>Package the hypothesis testing workflow from the lesson into a reusable, production-grade Python class called <code>StatisticalValidator</code>. This is the utility you will use in every future model comparison \u2014 and the foundation for P9: A/B Testing Statistical Framework.</p>","tags":["lab","statistics","month-01"]},{"location":"month-01/module-00-statistics/lab_statistical_validator/#the-statisticalvalidator-class","title":"The <code>StatisticalValidator</code> Class","text":"statistical_validator.py<pre><code>\"\"\"\nstatistical_validator.py\n========================\nReusable statistical comparison utility for ML model evaluation.\n\nUsage:\n    validator = StatisticalValidator(alpha=0.05)\n    result = validator.compare(scores_a, scores_b)\n    print(result)\n\nPart of: Utkarsh AI Labs \u2014 Month 1, Module 0\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nimport numpy as np\nfrom scipy import stats\n\n\n@dataclass\nclass ComparisonResult:\n    \"\"\"Structured result of a two-sample statistical comparison.\"\"\"\n\n    model_a_mean: float\n    model_b_mean: float\n    model_a_std: float\n    model_b_std: float\n    t_statistic: float\n    p_value: float\n    cohens_d: float\n    ci_lower: float\n    ci_upper: float\n    alpha: float\n    is_significant: bool\n    effect_magnitude: str  # \"negligible\", \"small\", \"medium\", \"large\"\n    recommendation: str\n\n    def __str__(self) -&gt; str:\n        return (\n            \"\\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\\n\"\n            \"\u2551       STATISTICAL COMPARISON REPORT          \u2551\\n\"\n            \"\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\\n\"\n            f\"\u2551 Model A \u2014 Mean: {self.model_a_mean:.4f} \u00b1 {self.model_a_std:.4f}       \u2551\\n\"\n            f\"\u2551 Model B \u2014 Mean: {self.model_b_mean:.4f} \u00b1 {self.model_b_std:.4f}       \u2551\\n\"\n            \"\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\\n\"\n            f\"\u2551 T-statistic:   {self.t_statistic:&gt;8.4f}                  \u2551\\n\"\n            f\"\u2551 P-value:       {self.p_value:&gt;8.4f}                  \u2551\\n\"\n            f\"\u2551 Cohen's d:     {self.cohens_d:&gt;8.4f} ({self.effect_magnitude})     \u2551\\n\"\n            f\"\u2551 95% CI:        [{self.ci_lower:.4f}, {self.ci_upper:.4f}]      \u2551\\n\"\n            \"\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\\n\"\n            f\"\u2551 Significant (\u03b1={self.alpha}): {'YES \u2713' if self.is_significant else 'NO \u2717':&gt;6}              \u2551\\n\"\n            f\"\u2551 Recommendation: {self.recommendation:&lt;28} \u2551\\n\"\n            \"\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\"\n        )\n\n\nclass StatisticalValidator:\n    \"\"\"\n    Compare two sets of model evaluation scores using\n    Welch's t-test, Cohen's d, and confidence intervals.\n\n    Parameters\n    ----------\n    alpha : float\n        Significance level (default: 0.05).\n    \"\"\"\n\n    def __init__(self, alpha: float = 0.05) -&gt; None:\n        if not 0 &lt; alpha &lt; 1:\n            raise ValueError(f\"alpha must be in (0, 1), got {alpha}\")\n        self.alpha = alpha\n\n    # ------------------------------------------------------------------ #\n    #  Public API                                                         #\n    # ------------------------------------------------------------------ #\n\n    def compare(\n        self,\n        scores_a: np.ndarray,\n        scores_b: np.ndarray,\n        model_a_name: Optional[str] = None,\n        model_b_name: Optional[str] = None,\n    ) -&gt; ComparisonResult:\n        \"\"\"\n        Run a full statistical comparison between two score arrays.\n\n        Parameters\n        ----------\n        scores_a : array-like\n            Evaluation scores for Model A (e.g., k-fold accuracies).\n        scores_b : array-like\n            Evaluation scores for Model B.\n\n        Returns\n        -------\n        ComparisonResult\n        \"\"\"\n        a = np.asarray(scores_a, dtype=float)\n        b = np.asarray(scores_b, dtype=float)\n\n        if len(a) &lt; 2 or len(b) &lt; 2:\n            raise ValueError(\"Each sample must have at least 2 observations.\")\n\n        # Welch's t-test\n        t_stat, p_val = stats.ttest_ind(a, b, equal_var=False)\n\n        # Effect size\n        d = self._cohens_d(a, b)\n        magnitude = self._effect_magnitude(d)\n\n        # Confidence interval for difference in means\n        ci_lo, ci_hi = self._confidence_interval(a, b)\n\n        # Decision\n        is_sig = p_val &lt; self.alpha\n        recommendation = self._recommend(is_sig, d, a.mean(), b.mean())\n\n        return ComparisonResult(\n            model_a_mean=a.mean(),\n            model_b_mean=b.mean(),\n            model_a_std=a.std(ddof=1),\n            model_b_std=b.std(ddof=1),\n            t_statistic=t_stat,\n            p_value=p_val,\n            cohens_d=d,\n            ci_lower=ci_lo,\n            ci_upper=ci_hi,\n            alpha=self.alpha,\n            is_significant=is_sig,\n            effect_magnitude=magnitude,\n            recommendation=recommendation,\n        )\n\n    # ------------------------------------------------------------------ #\n    #  Internal methods                                                   #\n    # ------------------------------------------------------------------ #\n\n    @staticmethod\n    def _cohens_d(a: np.ndarray, b: np.ndarray) -&gt; float:\n        n1, n2 = len(a), len(b)\n        pooled_std = np.sqrt(\n            ((n1 - 1) * a.std(ddof=1) ** 2 + (n2 - 1) * b.std(ddof=1) ** 2)\n            / (n1 + n2 - 2)\n        )\n        if pooled_std == 0:\n            return 0.0\n        return float((a.mean() - b.mean()) / pooled_std)\n\n    @staticmethod\n    def _effect_magnitude(d: float) -&gt; str:\n        d_abs = abs(d)\n        if d_abs &lt; 0.2:\n            return \"negligible\"\n        elif d_abs &lt; 0.5:\n            return \"small\"\n        elif d_abs &lt; 0.8:\n            return \"medium\"\n        return \"large\"\n\n    def _confidence_interval(\n        self, a: np.ndarray, b: np.ndarray\n    ) -&gt; tuple[float, float]:\n        diff = a.mean() - b.mean()\n        se = np.sqrt(a.var(ddof=1) / len(a) + b.var(ddof=1) / len(b))\n        df = len(a) + len(b) - 2\n        t_crit = stats.t.ppf(1 - self.alpha / 2, df)\n        return float(diff - t_crit * se), float(diff + t_crit * se)\n\n    @staticmethod\n    def _recommend(is_sig: bool, d: float, mean_a: float, mean_b: float) -&gt; str:\n        if not is_sig:\n            return \"Keep current model (A)\"\n        if abs(d) &lt; 0.2:\n            return \"Significant but negligible \u0394\"\n        better = \"B\" if mean_b &gt; mean_a else \"A\"\n        return f\"Deploy Model {better}\"\n\n\n# ================================================================== #\n#  Quick demo (runs when executed directly)                           #\n# ================================================================== #\nif __name__ == \"__main__\":\n    from sklearn.datasets import load_breast_cancer\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.model_selection import cross_val_score\n\n    X, y = load_breast_cancer(return_X_y=True)\n\n    scores_lr = cross_val_score(\n        LogisticRegression(max_iter=10_000, random_state=42),\n        X, y, cv=10, scoring=\"accuracy\",\n    )\n    scores_rf = cross_val_score(\n        RandomForestClassifier(n_estimators=100, random_state=42),\n        X, y, cv=10, scoring=\"accuracy\",\n    )\n\n    validator = StatisticalValidator(alpha=0.05)\n    result = validator.compare(scores_lr, scores_rf)\n    print(result)\n</code></pre>","tags":["lab","statistics","month-01"]},{"location":"month-01/module-00-statistics/lab_statistical_validator/#how-to-use-it","title":"How to Use It","text":"BasicStricter \u03b1 (Medical/Finance)With Custom Metric <pre><code>from statistical_validator import StatisticalValidator\nimport numpy as np\n\nscores_a = np.array([0.91, 0.89, 0.90, 0.92, 0.88, 0.91, 0.90, 0.89, 0.91, 0.90])\nscores_b = np.array([0.93, 0.92, 0.91, 0.94, 0.90, 0.93, 0.92, 0.91, 0.93, 0.92])\n\nvalidator = StatisticalValidator(alpha=0.05)\nresult = validator.compare(scores_a, scores_b)\nprint(result)\n</code></pre> <pre><code>validator = StatisticalValidator(alpha=0.01)\nresult = validator.compare(scores_a, scores_b)\nprint(f\"Significant at \u03b1=0.01? {result.is_significant}\")\n</code></pre> <pre><code>from sklearn.model_selection import cross_val_score\n\n# Use F1 instead of accuracy\nscores_a = cross_val_score(model_a, X, y, cv=10, scoring=\"f1\")\nscores_b = cross_val_score(model_b, X, y, cv=10, scoring=\"f1\")\n\nresult = validator.compare(scores_a, scores_b)\n</code></pre>","tags":["lab","statistics","month-01"]},{"location":"month-01/module-00-statistics/lab_statistical_validator/#acceptance-criteria","title":"Acceptance Criteria","text":"<ul> <li> Class is importable and passes <code>mypy --strict</code></li> <li> <code>ComparisonResult</code> is a dataclass with a human-readable <code>__str__</code></li> <li> Welch's t-test used (not Student's \u2014 never assume equal variance)</li> <li> Cohen's d computed and classified</li> <li> 95% CI for the mean difference reported</li> <li> Recommendation logic is deterministic and testable</li> </ul>","tags":["lab","statistics","month-01"]},{"location":"month-01/module-00-statistics/proof_of_work/","title":"Proof of Work \u2014 Module 0: Statistics &amp; Probability","text":"<p>COMMIT OR FAIL</p>","tags":["proof-of-work","challenge","statistics","month-01"]},{"location":"month-01/module-00-statistics/proof_of_work/#philosophy","title":"Philosophy","text":"<p>The Utkarsh AI Labs Rule</p> <p>Reading is not learning. Committing code is.</p> <p>If you can't build it, you don't understand it. This \"Proof of Work\" is your AI Clinic Equivalent \u2014 the artifact that proves comprehension under real constraints.</p>","tags":["proof-of-work","challenge","statistics","month-01"]},{"location":"month-01/module-00-statistics/proof_of_work/#the-challenge-statistical_validator-cli-tool","title":"The Challenge: <code>statistical_validator</code> CLI Tool","text":"","tags":["proof-of-work","challenge","statistics","month-01"]},{"location":"month-01/module-00-statistics/proof_of_work/#mission","title":"Mission","text":"<p>Build a command-line utility called <code>statistical_validator</code> that:</p> <ol> <li>Accepts two CSV files (or a single CSV with a group column) containing evaluation scores.</li> <li>Runs the full statistical comparison pipeline (Welch's t-test, Cohen's d, 95% CI).</li> <li>Outputs a structured report to <code>stdout</code> and saves a JSON artifact.</li> <li>Exits with code <code>0</code> if \\(H_0\\) is rejected (model improvement confirmed) or <code>1</code> if not.</li> </ol> <p>This tool will become a building block for P9: A/B Testing Statistical Framework.</p>","tags":["proof-of-work","challenge","statistics","month-01"]},{"location":"month-01/module-00-statistics/proof_of_work/#specification","title":"Specification","text":"<pre><code>statistical_validator compare \\\n    --scores-a results_model_a.csv \\\n    --scores-b results_model_b.csv \\\n    --alpha 0.05 \\\n    --output report.json\n</code></pre>","tags":["proof-of-work","challenge","statistics","month-01"]},{"location":"month-01/module-00-statistics/proof_of_work/#required-output-json","title":"Required Output (JSON)","text":"<pre><code>{\n  \"model_a\": {\n    \"mean\": 0.908,\n    \"std\": 0.012,\n    \"n\": 10\n  },\n  \"model_b\": {\n    \"mean\": 0.913,\n    \"std\": 0.014,\n    \"n\": 10\n  },\n  \"test\": {\n    \"name\": \"welch_t_test\",\n    \"t_statistic\": -0.8721,\n    \"p_value\": 0.2314,\n    \"alpha\": 0.05,\n    \"significant\": false\n  },\n  \"effect_size\": {\n    \"cohens_d\": -0.3826,\n    \"magnitude\": \"small\"\n  },\n  \"confidence_interval\": {\n    \"level\": 0.95,\n    \"lower\": -0.0142,\n    \"upper\": 0.0042\n  },\n  \"recommendation\": \"Keep current model (A)\"\n}\n</code></pre>","tags":["proof-of-work","challenge","statistics","month-01"]},{"location":"month-01/module-00-statistics/proof_of_work/#grading-rubric","title":"Grading Rubric","text":"Criterion Points Requirement Core Logic 30 Welch's t-test, Cohen's d, and CI are correctly computed CLI Interface 20 Uses <code>argparse</code> or <code>click</code>; handles missing files gracefully JSON Output 15 Matches the schema above; parseable by downstream tools Exit Codes 10 <code>0</code> = significant improvement, <code>1</code> = no improvement Tests 15 At least 3 unit tests using <code>pytest</code> (edge cases included) Documentation 10 README with usage examples and a \"Why this matters\" section Total 100 <p>Minimum passing score: 70/100</p> <p>You must implement Core Logic + CLI + JSON Output at minimum.</p>","tags":["proof-of-work","challenge","statistics","month-01"]},{"location":"month-01/module-00-statistics/proof_of_work/#required-unit-tests","title":"Required Unit Tests","text":"<p>Write at least these three test cases:</p> test_statistical_validator.py<pre><code>import numpy as np\nimport pytest\nfrom statistical_validator import StatisticalValidator\n\n\ndef test_identical_distributions_not_significant():\n    \"\"\"Two identical score sets should NOT be significant.\"\"\"\n    scores = np.array([0.90, 0.91, 0.89, 0.90, 0.92, 0.91, 0.90, 0.89, 0.91, 0.90])\n    validator = StatisticalValidator(alpha=0.05)\n    result = validator.compare(scores, scores)\n    assert result.is_significant is False\n    assert result.p_value == 1.0  # identical \u2192 p = 1\n\n\ndef test_clearly_different_distributions_significant():\n    \"\"\"Widely separated distributions should be significant.\"\"\"\n    a = np.array([0.50, 0.51, 0.49, 0.52, 0.50, 0.48, 0.51, 0.49, 0.50, 0.52])\n    b = np.array([0.95, 0.94, 0.96, 0.93, 0.95, 0.94, 0.96, 0.95, 0.94, 0.93])\n    validator = StatisticalValidator(alpha=0.05)\n    result = validator.compare(a, b)\n    assert result.is_significant is True\n    assert abs(result.cohens_d) &gt; 0.8  # large effect\n\n\ndef test_invalid_alpha_raises():\n    \"\"\"Alpha outside (0, 1) should raise ValueError.\"\"\"\n    with pytest.raises(ValueError):\n        StatisticalValidator(alpha=0.0)\n    with pytest.raises(ValueError):\n        StatisticalValidator(alpha=1.5)\n</code></pre>","tags":["proof-of-work","challenge","statistics","month-01"]},{"location":"month-01/module-00-statistics/proof_of_work/#stretch-goals-bonus","title":"Stretch Goals (Bonus)","text":"<ul> <li> Power Analysis: Add a <code>power</code> subcommand that computes required sample size given desired \\(\\alpha\\), power, and minimum detectable effect.</li> <li> Paired Test: Support <code>--paired</code> flag for paired t-tests (same dataset, different models).</li> <li> Visualization: Generate a box plot comparing the two distributions (save as PNG).</li> <li> CI/CD Integration: Write a GitHub Actions workflow that runs the validator on every PR.</li> </ul>","tags":["proof-of-work","challenge","statistics","month-01"]},{"location":"month-01/module-00-statistics/proof_of_work/#submission-checklist","title":"Submission Checklist","text":"<pre><code>your-repo/\n\u251c\u2500\u2500 statistical_validator.py      # Core class\n\u251c\u2500\u2500 cli.py                        # CLI entry point\n\u251c\u2500\u2500 test_statistical_validator.py  # pytest tests\n\u251c\u2500\u2500 requirements.txt              # scipy, numpy, scikit-learn\n\u251c\u2500\u2500 sample_data/\n\u2502   \u251c\u2500\u2500 scores_model_a.csv\n\u2502   \u2514\u2500\u2500 scores_model_b.csv\n\u2514\u2500\u2500 README.md                     # Usage + \"Why this matters\"\n</code></pre> <p>Definition of Done</p> <p>This module is complete when:</p> <ol> <li>All three unit tests pass (<code>pytest -v</code>).</li> <li>The CLI produces valid JSON output.</li> <li>The code is committed to your GitHub repo with the message:</li> </ol> <pre><code>feat(m0): add statistical validator \u2014 proof of work for Module 0\n</code></pre> <ol> <li>The commit SHA is logged in your Dashboard.</li> </ol>","tags":["proof-of-work","challenge","statistics","month-01"]},{"location":"month-01/module-00-statistics/proof_of_work/#why-this-matters-for-your-career","title":"Why This Matters for Your Career","text":"<p>This isn't a toy exercise. The <code>StatisticalValidator</code> pattern is used at:</p> <ul> <li>Netflix \u2014 for comparing recommendation model variants.</li> <li>Google \u2014 for evaluating search ranking changes.</li> <li>Any ML team \u2014 as a gate in the model promotion pipeline.</li> </ul> <p>When you sit in an interview and they ask \"How do you decide whether to deploy a new model?\", you won't recite a textbook definition. You'll say: \"I built a tool for that. Here's the commit.\"</p> <p>That's Unstoppable.</p>","tags":["proof-of-work","challenge","statistics","month-01"]}]}