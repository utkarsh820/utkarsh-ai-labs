{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Utkarsh AI Labs","text":"<p>A 6-month AI/ML learning academy. Theory, code, proof-of-work commits.</p> Month Focus 1 Mathematical Foundations (Linear Algebra, Probability, Statistics, Optimization, Python for ML, SQL Basics) 2 Classical ML &amp; Feature Engineering (Regression, Trees, SVM, Ensembles, Feature Selection, Model Evaluation, EDA, Advanced SQL) 3 Deep Learning &amp; NLP (Neural Networks, CNNs, RNNs, Transformers, BERT Fine-tuning, HuggingFace, Model Evaluation) 4 MLOps &amp; Deployment (FastAPI, Docker, MLflow, CI/CD, AWS/GCP Basics, Model Monitoring, Kubernetes Fundamentals) 5 GenAI &amp; LLM Engineering (RAG Pipelines, Vector Databases, LangChain/LangGraph, LoRA Fine-tuning, Multi-Agent Systems, LLM Evaluation) 6 Capstone &amp; Portfolio (Production AI System, End-to-End Deployment, Monitoring, Technical Blogging, GitHub Optimization, Job Applications &amp; Mock Interviews) <p>Current: Month 1 \u2014 Foundations \u00b7 Dashboard</p>"},{"location":"dashboard/","title":"DASHBOARD","text":""},{"location":"certifications/","title":"Certifications","text":""},{"location":"daily-schedule/","title":"Daily Schedule","text":""},{"location":"dsa-sql-practice/","title":"DSA &amp; SQL Practice","text":""},{"location":"month-01/","title":"Month 1 \u2014 Foundations","text":""},{"location":"month-01/#month-1-foundations_1","title":"Month 1 \u2014 Foundations","text":""},{"location":"month-01/week-01-python-git-linux/git/","title":"Git","text":""},{"location":"month-01/week-01-python-git-linux/linux/","title":"Linux","text":""},{"location":"month-01/week-01-python-git-linux/python/","title":"Python","text":""},{"location":"month-01/week-01-python-git-linux/python/#aiml-python-stack-mastery-index-0-elite","title":"AI/ML Python Stack Mastery Index (0 \u2192 Elite)","text":""},{"location":"month-01/week-01-python-git-linux/python/#1-core-python-programming","title":"1. Core Python Programming","text":"Topic Area Beginner (Foundations) Intermediate (Engineering) Advanced / Elite (Optimization &amp; Internals) Syntax &amp; Data Variables, Primitive Types, Type Conversion Type Hinting (<code>typing</code>), Data Classes (<code>dataclasses</code>) Metaclasses, Descriptors, <code>__slots__</code> for memory Control Flow If/Else, For/While Loops List/Dict Comprehensions, <code>zip</code>, <code>enumerate</code> Generators (<code>yield</code>), Coroutines (<code>async/await</code>) Functions Definition, Arguments, Return values <code>*args</code>, <code>**kwargs</code>, Lambda, Map/Filter Decorators (with args), Closures, Functools Object Oriented Classes, Objects, Basic Methods Inheritance, Polymorphism, Magic Methods (<code>__init__</code>) Abstract Base Classes, Metaprogramming, Mixins Modules &amp; Env Import, Pip install, Basic Scripts Virtual Environments (<code>venv</code>, <code>poetry</code>), Package structure Relative imports, Namespace packages, Wheel building File I/O Read/Write TXT, CSV, JSON Context Managers (<code>with</code>), Pathlib Memory Mapping (<code>mmap</code>), Binary protocols, Pickle safety Error Handling Try/Except, Basic Errors Custom Exceptions, Logging module Context propagation, Debugging profilers (<code>cProfile</code>) Concurrency Single-threaded scripts <code>threading</code> (I/O), <code>subprocess</code> <code>multiprocessing</code> (CPU), GIL bypass, AsyncIO loops Performance Writing working code Vectorization awareness, Algorithmic complexity C-Extensions (<code>PyBind11</code>), JIT (<code>Numba</code>), Memory profiling"},{"location":"month-01/week-01-python-git-linux/python/#2-numpy-numerical-computing","title":"2. NumPy (Numerical Computing)","text":"Topic Area Beginner (Foundations) Intermediate (Engineering) Advanced / Elite (Optimization &amp; Internals) Arrays <code>np.array</code>, <code>np.zeros</code>, <code>np.ones</code> <code>np.arange</code>, <code>np.linspace</code>, Data types (<code>dtype</code>) Structured arrays, Record arrays, Memory views Manipulation <code>reshape</code>, <code>flatten</code>, <code>transpose</code> <code>concatenate</code>, <code>stack</code>, <code>squeeze</code>, <code>expand_dims</code> <code>stride_tricks</code>, <code>as_strided</code>, In-place operations Indexing Basic slicing <code>[0:5]</code> Boolean masking, Fancy indexing <code>np.take</code>, <code>np.put</code>, Advanced stride manipulation Math Ops +, -, *, /, <code>np.sum</code>, <code>np.mean</code> <code>np.dot</code>, <code>np.matmul</code>, Aggregations (<code>axis</code>) <code>np.einsum</code>, Universal Functions (UFuncs), Broadcasting rules Linear Algebra <code>np.linalg.inv</code>, <code>np.linalg.det</code> Eigenvalues, SVD, Matrix Decomposition Sparse matrices (<code>scipy.sparse</code>), BLAS/LAPACK binding Random <code>np.random.rand</code>, <code>np.random.randint</code> <code>np.random.normal</code>, Seeds, Distributions <code>np.random.Generator</code>, PCG64, Parallel RNG Performance Python loops over arrays Vectorization (avoiding loops) Memory layout (C vs Fortran order), Cache locality Interoperability List to Array conversion Array to List, Pandas interop ctypes, C-API, Sharing memory with PyTorch/TensorFlow"},{"location":"month-01/week-01-python-git-linux/python/#3-pandas-data-manipulation","title":"3. Pandas (Data Manipulation)","text":"Topic Area Beginner (Foundations) Intermediate (Engineering) Advanced / Elite (Optimization &amp; Internals) Structures <code>Series</code>, <code>DataFrame</code> creation Index management, Column selection MultiIndex (Hierarchical), Categorical dtype I/O <code>read_csv</code>, <code>to_csv</code>, <code>read_excel</code> <code>read_sql</code>, <code>to_parquet</code>, Chunking (<code>chunksize</code>) Feather/Arrow format, Database connectors, S3 integration Cleaning <code>dropna</code>, <code>fillna</code>, <code>drop_duplicates</code> <code>replace</code>, <code>astype</code>, String methods (<code>str.</code>) Regex extraction, Custom NA handling, Memory reduction Selection <code>loc</code>, <code>iloc</code>, Boolean filtering <code>query</code>, <code>select_dtypes</code>, <code>isin</code> Index alignment, Reindexing, MultiIndex slicing Transformation <code>apply</code>, <code>map</code>, <code>applymap</code> Vectorized operations, <code>assign</code> Custom Accessors, <code>pipe</code>, GroupBy transformations Aggregation <code>sum</code>, <code>mean</code>, <code>count</code> <code>groupby</code>, <code>agg</code>, <code>pivot_table</code> <code>groupby</code> internals, Rolling/Expanding windows, Resampling Merging <code>concat</code>, <code>merge</code> (join) <code>join</code>, <code>combine_first</code>, Append Database-style joins, Overlapping columns, Validation Time Series <code>to_datetime</code>, Basic indexing Date ranges, Frequency conversion, Timezones Business days, Custom offsets, Lag/Lead features Performance Default dtypes Downcasting numerics, Categoricals <code>eval</code>/<code>query</code> engine, Swapping to Polars, Memory profiling"},{"location":"month-01/week-01-python-git-linux/python/#4-scikit-learn-machine-learning","title":"4. Scikit-Learn (Machine Learning)","text":"Topic Area Beginner (Foundations) Intermediate (Engineering) Advanced / Elite (Optimization &amp; Internals) API Standard <code>fit</code>, <code>predict</code>, <code>score</code> <code>fit_transform</code>, <code>inverse_transform</code> Custom Estimators, Transformers, Meta-estimators Preprocessing <code>StandardScaler</code>, <code>MinMaxScaler</code> <code>OneHotEncoder</code>, <code>Imputer</code>, <code>Pipeline</code> Custom Transformers, FeatureUnion, ColumnTransformer Models Linear/Logistic Regression, KNN Decision Trees, Random Forest, SVM Gradient Boosting (HistGradientBoosting), Ensemble voting Validation <code>train_test_split</code> <code>cross_val_score</code>, K-Fold, Stratified K-Fold TimeSeriesSplit, GroupKFold, Nested CV, Leakage prevention Metrics <code>accuracy_score</code>, <code>mean_squared_error</code> Precision/Recall, F1, ROC/AUC, Confusion Matrix Custom scorers, Multi-output metrics, Probability calibration Tuning Manual parameter change <code>GridSearchCV</code>, <code>RandomizedSearchCV</code> <code>HalvingGridSearch</code>, Bayesian Opt (integration), Early stopping Pipeline Basic sequential steps Feature selection within pipeline Parallel pipeline execution, Caching (<code>memory</code> param) Serialization <code>pickle</code> <code>joblib.dump</code>, <code>joblib.load</code> Model versioning, ONNX conversion, Sparse model storage Scaling Single machine, RAM fit <code>partial_fit</code> (Online learning) Distributed strategies, Out-of-core learning, Approximate nearest neighbors"},{"location":"month-01/week-01-python-git-linux/python/jupyter-demo/","title":"Jupyter Notebook Native Support","text":"In\u00a0[1]: Copied! <pre>print(\"Hello, MkDocs-Jupyter! Everything is working correctly.\")\n\ntopics = [\"Neural Networks\", \"Transformers\", \"Agentic Architectures\"]\nprint(\"List of dynamically added AI topics:\")\nfor topic in topics:\n    print(f\"- {topic}\")\n</pre> print(\"Hello, MkDocs-Jupyter! Everything is working correctly.\")  topics = [\"Neural Networks\", \"Transformers\", \"Agentic Architectures\"] print(\"List of dynamically added AI topics:\") for topic in topics:     print(f\"- {topic}\") <pre>Hello, MkDocs-Jupyter! Everything is working correctly.\nList of dynamically added AI topics:\n- Neural Networks\n- Transformers\n- Agentic Architectures\n</pre>"},{"location":"month-01/week-01-python-git-linux/python/jupyter-demo/#jupyter-notebook-native-support","title":"Jupyter Notebook Native Support\u00b6","text":"<p>This is a sample notebook proving that MkDocs can parse, render, and execute Jupyter Notebooks directly inside the academy website natively.</p> <p>You can now securely drop <code>.ipynb</code> files into any nested folder!</p>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/","title":"Descriptive Statistics Essentials Index (0 \u2192 Elite)","text":"Topic Area Beginner (Foundations) Intermediate (Engineering) Advanced / Elite (Optimization &amp; Internals) Central Tendency Mean, Median, Mode Weighted mean, Trimmed mean Geometric mean, Harmonic mean, Winsorized mean Dispersion Range, Variance, Standard Deviation Interquartile Range (IQR), MAD Coefficient of Variation, Robust scale estimators Position Min, Max, Percentiles Quartiles, Deciles Quantile functions, Order statistics Shape Skewness (concept) Kurtosis (concept) Moment-based shape analysis, Jarque-Bera test Distribution Visualization Histogram, Bar chart Box plot, Violin plot ECDF, Q-Q plot, Ridge plots, Heatmaps Frequency Tables Count, Simple frequency Relative frequency, Cumulative frequency Cross-tabulation, Contingency tables Data Types Numerical vs Categorical Discrete vs Continuous, Ordinal vs Nominal Interval vs Ratio, Mixed-type handling Outlier Detection Visual inspection IQR method (1.5\u00d7 rule) Z-score method, Modified Z-score, Isolation Forest Missing Data Count missing values Missing percentage, Patterns MCAR/MAR/MNAR classification, Missingness heatmap Data Quality Duplicate detection Unique value counts Data validation rules, Schema enforcement Aggregation Sum, Count, Mean GroupBy aggregations Multi-level aggregation, Custom agg functions Bivariate Analysis Scatter plot, Side-by-side bars Covariance, Correlation matrix Pairwise relationships, Conditional distributions Multivariate Analysis \u2014 Pair plots, Correlation heatmaps PCA for exploration, Parallel coordinates Time-Based Descriptives Time period counts Trend lines, Moving averages Seasonal decomposition, Lag features Categorical Summaries Frequency, Mode Proportions, Percentages Entropy, Diversity indices, Mode stability Numerical Summaries 5-number summary Describe() output Extended summaries, Custom summary stats Data Transformation Log, Square root Standardization (Z-score), Normalization (Min-Max) Box-Cox, Yeo-Johnson, Rank transformation Binning Equal-width bins Equal-frequency bins Optimal binning (Sturges, Freedman-Diaconis) Summary Tables Basic stat tables Grouped summary tables Multi-index summary, Styled reports Reporting Basic statistics output Formatted tables (rounding) Automated reports, APA-style tables, Dashboards"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/#quick-reference-descriptive-stats-by-data-type","title":"Quick Reference: Descriptive Stats by Data Type","text":"Data Type Central Tendency Dispersion Visualization Numerical (Normal) Mean Std Dev, Variance Histogram, Box plot Numerical (Skewed) Median IQR, MAD Box plot, Violin plot Ordinal Median, Mode Range, Percentiles Bar chart, Cumulative freq Nominal Mode Frequency, Proportion Bar chart, Pie chart Time Series Moving average Rolling std Line plot, Area chart"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/#essential-python-libraries","title":"Essential Python Libraries","text":"Library Purpose Key Functions <code>pandas</code> Data summarization <code>describe()</code>, <code>agg()</code>, <code>groupby()</code>, <code>value_counts()</code> <code>numpy</code> Numerical operations <code>mean()</code>, <code>std()</code>, <code>percentile()</code>, <code>histogram()</code> <code>scipy.stats</code> Distribution stats <code>skew()</code>, <code>kurtosis()</code>, <code>describe()</code> <code>matplotlib</code> Basic visualization <code>hist()</code>, <code>boxplot()</code>, <code>scatter()</code> <code>seaborn</code> Statistical visualization <code>distplot()</code>, <code>boxplot()</code>, <code>pairplot()</code>, <code>heatmap()</code> <code>pingouin</code> Extended descriptives <code>describe()</code>, <code>summary_stats()</code>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/#key-formulas-reference","title":"Key Formulas Reference","text":"Measure Formula Python Equivalent Mean \u03a3x / n <code>np.mean()</code> Median Middle value <code>np.median()</code> Variance \u03a3(x - mean)\u00b2 / (n-1) <code>np.var(ddof=1)</code> Std Deviation \u221aVariance <code>np.std(ddof=1)</code> IQR Q3 - Q1 <code>np.percentile(75) - np.percentile(25)</code> Skewness E[(x-\u03bc)\u00b3] / \u03c3\u00b3 <code>scipy.stats.skew()</code> Kurtosis E[(x-\u03bc)\u2074] / \u03c3\u2074 - 3 <code>scipy.stats.kurtosis()</code> CV (Std Dev / Mean) \u00d7 100 <code>(std/mean)*100</code> Z-Score (x - mean) / std <code>scipy.stats.zscore()</code>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/#5-number-summary-checklist","title":"5-Number Summary Checklist","text":"Statistic Description Python Minimum Smallest value <code>df.min()</code> Q1 (25th) Lower quartile <code>df.quantile(0.25)</code> Median (50th) Middle value <code>df.median()</code> Q3 (75th) Upper quartile <code>df.quantile(0.75)</code> Maximum Largest value <code>df.max()</code>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/01-central-tendency/","title":"Central Tendency","text":"<p>Central tendency measures describe the \"center\" or \"typical\" value of a dataset. Understanding these is the fundamental first step in any exploratory data analysis.</p>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/01-central-tendency/#beginner-foundations","title":"\ud83d\udfe2 Beginner (Foundations)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/01-central-tendency/#1-mean-arithmetic-average","title":"1. Mean (Arithmetic Average)","text":"<p>The mean is the sum of all values divided by the number of values. It is highly sensitive to outliers.</p> <ul> <li>Formula: \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i\\)</li> <li>Python Implementation:</li> </ul> <pre><code>import pandas as pd\nimport numpy as np\n\ndata = [10, 20, 30, 40, 50, 1000] # 1000 is an outlier\nmean_val = np.mean(data)\nprint(f\"Mean: {mean_val}\") # Output: 191.66\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/01-central-tendency/#2-median","title":"2. Median","text":"<p>The median is the middle value when the data is sorted. It is robust (not heavily affected by outliers).</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>median_val = np.np.median(data)\nprint(f\"Median: {median_val}\") # Output: 35.0\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/01-central-tendency/#3-mode","title":"3. Mode","text":"<p>The mode is the most frequently occurring value. Useful specifically for categorical data.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>from scipy import stats\nmode_val = stats.mode([1, 2, 2, 3, 4], keepdims=True)\nprint(f\"Mode: {mode_val.mode[0]}\") # Output: 2\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/01-central-tendency/#intermediate-engineering","title":"\ud83d\udfe1 Intermediate (Engineering)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/01-central-tendency/#1-weighted-mean","title":"1. Weighted Mean","text":"<p>Used when certain data points contribute more \"weight\" or importance than others (e.g., calculating GPA from credits).</p> <ul> <li>Formula: \\(\\bar{x}_w = \\frac{\\sum_{i=1}^{n}w_i x_i}{\\sum_{i=1}^{n}w_i}\\)</li> <li>Python Implementation:</li> </ul> <pre><code>values = [80, 90, 95]\nweights = [0.2, 0.3, 0.5] # Exams weights\nweighted_mean = np.average(values, weights=weights)\nprint(f\"Weighted Mean: {weighted_mean}\") # Output: 90.5\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/01-central-tendency/#2-trimmed-mean","title":"2. Trimmed Mean","text":"<p>A compromise between the mean and median. It involves discarding a certain percentage of the lowest and highest values before calculating the mean, explicitly removing outliers.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>from scipy.stats import trim_mean\n# Trim 10% from both ends\ntrimmed = trim_mean([1, 2, 3, 4, 5, 6, 7, 8, 9, 1000], proportiontocut=0.1)\nprint(f\"Trimmed Mean: {trimmed}\") # Output: 5.5\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/01-central-tendency/#advanced-elite-optimization","title":"\ud83d\udd34 Advanced / Elite (Optimization)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/01-central-tendency/#1-geometric-mean","title":"1. Geometric Mean","text":"<p>Used for data that grows multiplicatively (e.g., compounding interest rates, investment returns, or biological cell growth). It answers: \"What is the constant growth rate?\"</p> <ul> <li>Formula: \\(GM = \\sqrt[n]{x_1 \\cdot x_2 \\cdots x_n}\\)</li> <li>Python Implementation:</li> </ul> <pre><code>from scipy.stats import gmean\nreturns = [1.05, 1.10, 0.95] # 5% gain, 10% gain, 5% loss\ngeom_mean = gmean(returns)\nprint(f\"Geometric Mean: {geom_mean}\")\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/01-central-tendency/#2-harmonic-mean","title":"2. Harmonic Mean","text":"<p>Used when calculating the average of rates or ratios (e.g., average speed given distance, or P/E ratios in finance).</p> <ul> <li>Formula: \\(HM = \\frac{n}{\\sum_{i=1}^{n}\\frac{1}{x_i}}\\)</li> <li>Python Implementation:</li> </ul> <pre><code>from scipy.stats import hmean\nspeeds = [60, 40] # Outward at 60mph, return at 40mph over same distance\nhar_mean = hmean(speeds)\nprint(f\"Harmonic Mean (Average Speed): {har_mean}\") # Output: 48.0\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/01-central-tendency/#3-winsorized-mean","title":"3. Winsorized Mean","text":"<p>Instead of trimming (removing) outliers, Winsorizing replaces extreme values with the nearest \"acceptable\" value (e.g., capping the 99th percentile). This retains the data point count but reduces variance explosion.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>from scipy.stats import mstats\n# Cap bottom 5% and top 5%\ndata = [1, 2, 3, 4, 5, 6, 7, 8, 9, 100]\nwinsorized_data = mstats.winsorize(data, limits=[0.05, 0.05])\nprint(f\"Winsorized Mean: {np.mean(winsorized_data)}\")\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/","title":"Dispersion and Shape","text":"<p>While Central Tendency tells us where the data \"lives\", Dispersion tells us how \"spread out\" it is. Shape tells us if the data leans heavily to one side (Skewness) or has heavy tails (Kurtosis).</p>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/#beginner-foundations","title":"\ud83d\udfe2 Beginner (Foundations)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/#1-range","title":"1. Range","text":"<p>The absolute difference between the largest and smallest values. It is completely susceptible to outliers.</p> <ul> <li>Formula: \\(Range = Max(x) - Min(x)\\)</li> <li>Python Implementation:</li> </ul> <pre><code>import numpy as np\ndata = [10, 20, 30, 40, 50]\nrange_val = np.ptp(data) # ptp stands for \"peak to peak\"\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/#2-variance-standard-deviation","title":"2. Variance &amp; Standard Deviation","text":"<p>Variance measures the average squared distance from the mean. Standard Deviation (\\(\\sigma\\)) takes the square root of Variance to return the metric back to its original units (e.g., dollars\u00b2 back to dollars).</p> <ul> <li>Formula (Sample Variance): \\(s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n - 1}\\)</li> <li>Python Implementation:</li> </ul> <pre><code>variance = np.var(data, ddof=1) # ddof=1 makes it a Sample Variance\nstd_dev = np.std(data, ddof=1)\nprint(f\"Variance: {variance}, Standard Deviation: {std_dev}\")\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/#3-skewness-concept","title":"3. Skewness (Concept)","text":"<p>Skewness measures the asymmetry of a distribution around its mean.</p> <ul> <li>Positive Skew (Right-skewed): Tail is on the right side. Mean &gt; Median &gt; Mode. (e.g., Household income)</li> <li>Negative Skew (Left-skewed): Tail is on the left side. Mode &gt; Median &gt; Mean. (e.g., Age of retirement)</li> </ul>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/#intermediate-engineering","title":"\ud83d\udfe1 Intermediate (Engineering)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/#1-interquartile-range-iqr","title":"1. Interquartile Range (IQR)","text":"<p>A robust measure of spread. It calculates the range of the middle 50% of the data, completely ignoring the top 25% and bottom 25% outliers.</p> <ul> <li>Formula: \\(IQR = Q_3 - Q_1\\) (75th percentile minus 25th percentile)</li> <li>Python Implementation:</li> </ul> <pre><code>q3, q1 = np.percentile(data, [75, 25])\niqr = q3 - q1\nprint(f\"IQR: {iqr}\")\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/#2-mean-absolute-deviation-mad","title":"2. Mean Absolute Deviation (MAD)","text":"<p>Instead of squaring distances like Variance does (which penalizes extreme outliers heavily), MAD simply takes the absolute value of the distances from the median. Highly robust.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>median = np.median(data)\nmad = np.median(np.abs(data - median))\nprint(f\"Robust MAD: {mad}\")\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/#3-kurtosis-concept","title":"3. Kurtosis (Concept)","text":"<p>Kurtosis measures the \"tailedness\" of the data distribution compared to a Normal Distribution. High kurtosis indicates heavy tails (high likelihood of extreme outliers).</p> <ul> <li>Mesokurtic: Normal distribution behavior (Kurtosis ~3, Excess Kurtosis ~0)</li> <li>Leptokurtic: Heavy tails, sharp peak (Excess Kurtosis &gt; 0)</li> <li>Platykurtic: Light tails, flat peak (Excess Kurtosis &lt; 0)</li> </ul>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/#advanced-elite-optimization","title":"\ud83d\udd34 Advanced / Elite (Optimization)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/#1-coefficient-of-variation-cv","title":"1. Coefficient of Variation (CV)","text":"<p>The ratio of the standard deviation to the mean. It allows you to objectively compare the \"risk\" or volatility of two completely different datasets (like comparing the volatility of a $10 stock vs. a $1000 stock).</p> <ul> <li>Formula: \\(CV = (\\frac{\\sigma}{\\mu}) \\times 100\\)</li> <li>Python Implementation:</li> </ul> <pre><code>mean_val = np.mean(data)\nstd_val = np.std(data, ddof=1)\ncv = (std_val / mean_val) * 100\nprint(f\"Coefficient of Variation: {cv}%\")\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/#2-robust-scale-estimators-rousseeuws-sn-and-qn","title":"2. Robust Scale Estimators (Rousseeuw's Sn and Qn)","text":"<p>In quantitative finance and elite data science, standard deviation breaks instantly. IQR is better but inefficient. <code>Sn</code> and <code>Qn</code> are highly efficient, highly robust estimators of scale that survive up to a 50% contamination rate in your dataset.</p> <ul> <li>Python Implementation (using <code>statsmodels</code>):</li> </ul> <pre><code>import statsmodels.robust.scale as robust\nrc = robust.mad(data) # Normalized MAD\nprint(f\"Robust Scale: {rc}\")\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/02-dispersion-and-shape/#3-moment-based-shape-analysis-moments-1-4","title":"3. Moment-Based Shape Analysis (Moments 1-4)","text":"<p>In probability theory:</p> <ul> <li>1st Moment (Mean): Location</li> <li>2nd Central Moment (Variance): Scale</li> <li>3rd Standardized Moment: Skewness</li> <li>4th Standardized Moment: Kurtosis</li> </ul> <pre><code>from scipy.stats import moment, skew, kurtosis\nm1 = moment(data, moment=1)\nm2 = moment(data, moment=2)\n# Fisher defined normal kurtosis as 0 (Excess Kurtosis)\nkurt = kurtosis(data, fisher=True)\nprint(f\"Skew: {skew(data)}, Excess Kurt: {kurt}\")\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/03-visualization-and-frequency/","title":"Visualization and Frequency","text":"<p>Exploratory Data Analysis (EDA) relies on human visual perception to instantly detect shapes, modalities, and relationships that raw numbers obscure. Frequency tables structure those visual bins locally.</p>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/03-visualization-and-frequency/#beginner-foundations","title":"\ud83d\udfe2 Beginner (Foundations)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/03-visualization-and-frequency/#1-histograms-vs-bar-charts","title":"1. Histograms vs. Bar Charts","text":"<ul> <li>Histograms: Used for Numerical/Continuous quantitative data. They bin raw numbers together into ranges (e.g., Age 10-20, 20-30). Bars touch strictly because data is continuous.</li> <li>Bar Charts: Used for Categorical/Discrete qualitative data. Bars do not touch because categories are discrete (e.g., Red, Blue, Green).</li> </ul> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndata = [12, 15, 14, 22, 28, 30, 31]\nsns.histplot(data, bins=3)\nplt.title(\"Continuous Data Binned\")\nplt.show()\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/03-visualization-and-frequency/#2-count-simple-frequency-tables","title":"2. Count &amp; Simple Frequency Tables","text":"<p>Finding how many times distinct values occur.</p> <pre><code>import pandas as pd\ndf = pd.DataFrame({\"color\": [\"red\", \"blue\", \"red\", \"green\", \"blue\", \"blue\"]})\nprint(df[\"color\"].value_counts()) # Returns raw counts\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/03-visualization-and-frequency/#intermediate-engineering","title":"\ud83d\udfe1 Intermediate (Engineering)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/03-visualization-and-frequency/#1-box-plots-and-violin-plots","title":"1. Box Plots and Violin Plots","text":"<ul> <li>Box Plot: A literal, visual manifestation of the 5-number summary (Min, Q1, Median, Q3, Max) and IQR. Dots outside the \"whiskers\" are mathematical outliers explicitly.</li> <li>Violin Plot: Combines a Box Plot with a Kernel Density Estimate (KDE). It solves the \"bimodality\" problem: A box plot can hide if a distribution has two distinct peaks, but a violin plot shows the exact curvature.</li> </ul> <pre><code>sns.violinplot(x=data)\nplt.title(\"Violin Plot displaying KDE Density Curve\")\nplt.show()\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/03-visualization-and-frequency/#2-relative-and-cumulative-frequency","title":"2. Relative and Cumulative Frequency","text":"<ul> <li>Relative Frequency: Turns raw counts into percentages of the whole population. <code>count / total_count</code>.</li> <li>Cumulative Frequency: A running total. Adds percentages sequentially until 100% is reached.</li> </ul> <pre><code>counts = df[\"color\"].value_counts(normalize=True) * 100\ncumulative = counts.cumsum()\nprint(\"Relative:\\n\", counts)\nprint(\"Cumulative:\\n\", cumulative)\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/03-visualization-and-frequency/#advanced-elite-optimization","title":"\ud83d\udd34 Advanced / Elite (Optimization)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/03-visualization-and-frequency/#1-empirical-cumulative-distribution-function-ecdf","title":"1. Empirical Cumulative Distribution Function (ECDF)","text":"<p>Rather than binning data like a histogram does (which forces you to subjectively choose the <code>bins=</code> parameter), an ECDF plots every single exact data point. It represents the proportion of items less than or equal to <code>x</code>. Fast, objective, and immune to binning bias.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>sns.ecdfplot(data)\nplt.title(\"Exact Cumulative Ratios\")\nplt.show()\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/03-visualization-and-frequency/#2-q-q-plots-quantile-quantile","title":"2. Q-Q Plots (Quantile-Quantile)","text":"<p>The absolute best way to check if your data actually follows a Normal Distribution. It compares your data's quantiles against a theoretical perfect normal distribution. If the data points hug the 45-degree diagonal red line, your data is perfectly normal.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>import scipy.stats as stats\nstats.probplot(data, dist=\"norm\", plot=plt)\nplt.title(\"Q-Q Plot\")\nplt.show()\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/03-visualization-and-frequency/#3-cross-tabulation-contingency-tables","title":"3. Cross-Tabulation &amp; Contingency Tables","text":"<p>Used to quantify the interaction rate between two entirely different categorical variables simultaneously.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>df[\"size\"] = [\"S\", \"M\", \"L\", \"S\", \"M\", \"L\"] # Merging with colors\ncontingency = pd.crosstab(df[\"color\"], df[\"size\"], normalize='index')\nprint(contingency)\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/04-data-quality-and-outliers/","title":"Data Quality and Outliers","text":"<p>Before any machine learning model is trained, the data must be sanitized. Statistical tests must be deployed to handle null-values (missingness) and detect anomalies.</p>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/04-data-quality-and-outliers/#beginner-foundations","title":"\ud83d\udfe2 Beginner (Foundations)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/04-data-quality-and-outliers/#1-visual-inspection-simple-counts","title":"1. Visual Inspection &amp; Simple Counts","text":"<p>Before running algorithms, look at the data structure. Find missing (<code>NaN</code>) values and simple duplicates.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\"Age\": [25, np.nan, 22, 25, 29, 300]}) # Exteme outlier and duplicate\n\nprint(\"Missing Values:\\n\", df.isnull().sum())\nprint(\"Duplicates:\\n\", df.duplicated().sum())\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/04-data-quality-and-outliers/#2-basic-deduplication","title":"2. Basic Deduplication","text":"<p>Removing exact identical rows safely.</p> <pre><code>df_cleaned = df.drop_duplicates()\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/04-data-quality-and-outliers/#intermediate-engineering","title":"\ud83d\udfe1 Intermediate (Engineering)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/04-data-quality-and-outliers/#1-missingness-percentage-matrix","title":"1. Missingness Percentage &amp; Matrix","text":"<p>Instead of raw counts (which are useless if you don't know the dataset size), calculate the percentage of missing values.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>missing_percent = (df.isnull().sum() / len(df)) * 100\nprint(missing_percent, \"% missing\")\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/04-data-quality-and-outliers/#2-iqr-outlier-method-tukeys-15-rule","title":"2. IQR Outlier Method (Tukey's 1.5\u00d7 Rule)","text":"<p>Any data point outside the boundary of \\((Q1 - 1.5 \\times IQR)\\) to \\((Q3 + 1.5 \\times IQR)\\) is mathematically flagged as an outlier. Very robust for skewed data.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>q1 = df[\"Age\"].quantile(0.25)\nq3 = df[\"Age\"].quantile(0.75)\niqr = q3 - q1\n\nlower_bound = q1 - 1.5 * iqr\nupper_bound = q3 + 1.5 * iqr\n\noutliers = df[(df[\"Age\"] &lt; lower_bound) | (df[\"Age\"] &gt; upper_bound)]\nprint(\"Flagged Outliers:\\n\", outliers)\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/04-data-quality-and-outliers/#advanced-elite-optimization","title":"\ud83d\udd34 Advanced / Elite (Optimization)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/04-data-quality-and-outliers/#1-the-z-score-outlier-method","title":"1. The Z-Score Outlier Method","text":"<p>Mathematically calculates how many standard deviations away a data point is from the mean. Typically, any absolute Z-score \\(&gt; 3\\) is considered an anomaly. Note: Z-scores require your data to be Normally Distributed and are highly sensitive to extreme outliers because the mean is used in the calculation.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>from scipy import stats\ndf_nomissing = df.dropna()\nz_scores = np.abs(stats.zscore(df_nomissing[\"Age\"]))\noutliers_z = df_nomissing[z_scores &gt; 3]\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/04-data-quality-and-outliers/#2-statistical-missingness-classification-mcar-mar-mnar","title":"2. Statistical Missingness Classification (MCAR, MAR, MNAR)","text":"<p>You cannot simply \"impute\" missing data with the mean blindly. You must scientifically classify why it is missing.</p> <ul> <li>Missing Completely at Random (MCAR): A true random glitch. Coin flip. (Safe to mean-impute or drop).</li> <li>Missing at Random (MAR): Missingness relies on a different observed variable. (e.g., Men are less likely to fill out depression surveys than women. We can predict missingness using the \"Gender\" column). (Use Multiple Imputation, MICE).</li> <li>Missing Not at Random (MNAR): Missingness relies on the unobserved variable itself. (e.g., People with the lowest incomes leave the \"Income\" column blank because they are embarrassed). (Highly dangerous. Requires specialized domain knowledge).</li> </ul>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/04-data-quality-and-outliers/#3-isolation-forest-unsupervised-anomaly-detection","title":"3. Isolation Forest (Unsupervised Anomaly Detection)","text":"<p>Algorithms like Isolation Forests explicitly isolate anomalies by randomly partitioning trees. Anomalies travel shorter paths than standard observations. Very powerful for multivariable datasets.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>from sklearn.ensemble import IsolationForest\n\nmodel = IsolationForest(contamination=0.05)\npredictions = model.fit_predict(df_nomissing[[\"Age\"]])\n# -1 indicates an outlier anomaly\ndf_nomissing[\"Anomaly\"] = predictions\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/05-bivariate-and-multivariate/","title":"Bivariate and Multivariate","text":"<p>Moving beyond single variables (univariate) to explore how two or more variables interact with one another simultaneously.</p>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/05-bivariate-and-multivariate/#beginner-foundations","title":"\ud83d\udfe2 Beginner (Foundations)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/05-bivariate-and-multivariate/#1-scatter-plots","title":"1. Scatter Plots","text":"<p>The fundamental plotting technique to see the raw visual relationship between two numerical variables.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = sns.load_dataset(\"tips\")\n\nsns.scatterplot(x=\"total_bill\", y=\"tip\", data=df)\nplt.title(\"Total Bill vs. Tip Amount\")\nplt.show()\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/05-bivariate-and-multivariate/#2-side-by-side-bar-charts","title":"2. Side-by-Side Bar Charts","text":"<p>Comparing continuous outputs across multiple different categories visually.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>sns.barplot(x=\"day\", y=\"total_bill\", hue=\"smoker\", data=df)\nplt.title(\"Bill by Day and Smoker Status\")\nplt.show()\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/05-bivariate-and-multivariate/#intermediate-engineering","title":"\ud83d\udfe1 Intermediate (Engineering)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/05-bivariate-and-multivariate/#1-covariance-vs-correlation","title":"1. Covariance vs. Correlation","text":"<ul> <li>Covariance: Measures the direction of the relationship between two variables. Very difficult to interpret because the scale relies entirely on the original data units (e.g., measuring height in miles vs. inches changes covariance completely).</li> <li>Correlation (Pearson's \\(r\\)): The standardized version of covariance. It objectively bounds the relationship strictly between <code>-1</code> (perfect inverse) and <code>+1</code> (perfect positive). A value of <code>0</code> means absolutely zero linear relationship.</li> </ul> <pre><code>correlation = df[[\"total_bill\", \"tip\"]].corr()\nprint(\"Pearson Correlation:\\n\", correlation)\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/05-bivariate-and-multivariate/#2-correlation-matrices-and-heatmaps","title":"2. Correlation Matrices and Heatmaps","text":"<p>Applying the underlying \\(r\\)-correlation formula to every single numerical column against every other numerical column simultaneously to build a matrix.</p> <ul> <li>Python Implementation:</li> </ul> <pre><code>numeric_df = df.select_dtypes(include=['float64', 'int64'])\nmatrix = numeric_df.corr()\n\nsns.heatmap(matrix, annot=True, cmap=\"coolwarm\")\nplt.title(\"Correlation Heatmap\")\nplt.show()\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/05-bivariate-and-multivariate/#3-pair-plots","title":"3. Pair Plots","text":"<p>Creates a grid of scatterplots for every single pair of variables while drawing the histogram distributions diagonally.</p> <pre><code>sns.pairplot(df, hue=\"sex\")\nplt.show()\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/05-bivariate-and-multivariate/#advanced-elite-optimization","title":"\ud83d\udd34 Advanced / Elite (Optimization)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/05-bivariate-and-multivariate/#1-pairwise-non-linear-relationships-spearman-kendall","title":"1. Pairwise Non-Linear Relationships (Spearman &amp; Kendall)","text":"<p>Pearson only detects linear relationships (straight lines). If <code>y = x^2</code> (an exponential curve), Pearson might read <code>0.0</code>.</p> <ul> <li>Spearman's Rho: Looks strictly at the rank order of variables. If variable X goes up, does variable Y go up (regardless of how fast)?</li> <li>Kendall's Tau: Excellent for incredibly small sample sizes where ranking is critical.</li> </ul> <pre><code>spearman_corr = df[[\"total_bill\", \"tip\"]].corr(method=\"spearman\")\nprint(\"Spearman Rank Correlation:\\n\", spearman_corr)\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/05-bivariate-and-multivariate/#2-pca-principal-component-analysis-for-eda-exploration","title":"2. PCA (Principal Component Analysis) for EDA Exploration","text":"<p>When a dataset has 50 columns, you cannot possibly visualize 50 dimensions on a flat screen or find simple pair plots. PCA is an extremely powerful linear algebra technique (SVD) applied to rotationally project 50 dimensions flat onto a 2D or 3D graph (the \"Components\" capturing the maximum variance).</p> <pre><code>from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Standardize data strictly first before PCA\nscaler = StandardScaler()\nscaled = scaler.fit_transform(numeric_df)\n\npca = PCA(n_components=2)\nprincipal_components = pca.fit_transform(scaled)\n\nplt.scatter(principal_components[:, 0], principal_components[:, 1])\nplt.xlabel(\"First Principal Component\")\nplt.ylabel(\"Second Principal Component\")\nplt.title(\"2D Projection of High-Dimensional Variance\")\nplt.show()\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/","title":"Advanced Summaries","text":"<p>Transforming data shapes into workable ML structures, generating grouped aggregations, and standardizing values for modeling calculations.</p>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#beginner-foundations","title":"\ud83d\udfe2 Beginner (Foundations)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#1-simple-data-transformations-log-square-root","title":"1. Simple Data Transformations (Log, Square Root)","text":"<p>Highly skewed data breaks linear assumptions in machine learning. We use static mathematical transformations to compress those massive outliers and force the distribution closer to a Bell Curve/Normal structure.</p> <ul> <li>Log Transform: \\(y = \\log(x+1)\\) (The \\(+1\\) handles datasets with exact zeros).</li> <li>Square Root Transform: \\(y = \\sqrt{x}\\)</li> </ul> <pre><code>import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\ndf = sns.load_dataset(\"diamonds\")\n\n# Log transform the heavily right-skewed price column\ndf[\"log_price\"] = np.log1p(df[\"price\"])\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#2-basic-grouping-and-tables","title":"2. Basic Grouping and Tables","text":"<p>Aggregating numbers by discrete categories.</p> <pre><code>mean_price_by_cut = df.groupby(\"cut\")[\"price\"].mean()\nprint(mean_price_by_cut)\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#3-equal-width-binning","title":"3. Equal-width Binning","text":"<p>Slicing continuous numerical data into discrete categories by explicitly typing ranges (e.g. \\([0 \\to 10]\\), \\([10 \\to 20]\\)).</p> <pre><code>df[\"price_category\"] = pd.cut(df[\"price\"], bins=3, labels=[\"Low\", \"Med\", \"High\"])\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#intermediate-engineering","title":"\ud83d\udfe1 Intermediate (Engineering)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#1-standardization-z-score","title":"1. Standardization (Z-score)","text":"<p>Transforms continuous data so its <code>Mean = 0</code> and its <code>Standard Deviation = 1</code>. Vital for neural networks, SVMs, Ridge regression, and PCA visualization where large numbers mathematically overpower small numbers.</p> <ul> <li>Formula: \\(Z = \\frac{x - \\mu}{\\sigma}\\)</li> </ul> <pre><code>from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndf[\"scaled_price\"] = scaler.fit_transform(df[[\"price\"]])\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#2-normalization-min-max","title":"2. Normalization (Min-Max)","text":"<p>Forcibly compresses the entire dataset perfectly between <code>0</code> and <code>1</code>. Highly sensitive to extreme outliers because the <code>Max</code> anchors all other values. Used heavily for Image/Pixel data in ConvNets.</p> <ul> <li>Formula: \\(X_{norm} = \\frac{x - x_{min}}{x_{max} - x_{min}}\\)</li> </ul>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#3-equal-frequency-binning-quantile-binning","title":"3. Equal-frequency Binning (Quantile Binning)","text":"<p>Unlike equal-width binning, this ensures every single bin contains the exact same number of samples, avoiding massive class imbalances.</p> <pre><code># 4 bins (quartiles), each receiving 25% of the data exactly\ndf[\"price_quantile\"] = pd.qcut(df[\"price\"], q=4, labels=[\"Q1\", \"Q2\", \"Q3\", \"Q4\"])\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#4-groupby-aggregations-with-multiple-functions","title":"4. GroupBy Aggregations with Multiple Functions","text":"<pre><code>summary = df.groupby(\"cut\").agg({\n    \"price\": [\"mean\", \"median\", \"std\"],\n    \"carat\": [\"max\"]\n})\nprint(summary)\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#advanced-elite-optimization","title":"\ud83d\udd34 Advanced / Elite (Optimization)","text":""},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#1-optimal-binning-freedman-diaconis-rule","title":"1. Optimal Binning (Freedman-Diaconis Rule)","text":"<p>Instead of guessing <code>bins=30</code> subjectively in a histogram, mathematically optimize the bin width to perfectly represent the underlying density, adjusting rigidly for sample size and IQR to ignore extreme outliers plotting themselves.</p> <ul> <li>Formula: \\(Bin\\_Width = 2 \\times \\frac{IQR(x)}{\\sqrt[3]{n}}\\)</li> </ul> <pre><code>import matplotlib.pyplot as plt\n\n# Numpy's 'fd' string natively applies Freedman-Diaconis equation\nplt.hist(df[\"price\"], bins=\"fd\")\nplt.title(\"Freedman-Diaconis Optimized Bins\")\nplt.show()\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#2-power-transformations-box-cox-yeo-johnson","title":"2. Power Transformations (Box-Cox &amp; Yeo-Johnson)","text":"<p>Instead of \"guessing\" whether to use Log or Square Root, these are advanced algorithms that rigorously hunt for the exact mathematical \\(\\lambda\\) exponent that transforms your specific arbitrary data point structure into a perfect Gaussian Normal Distribution.</p> <ul> <li>Box-Cox: Data strictly must be positive ( \\(&gt; 0\\)).</li> <li>Yeo-Johnson: Elegantly handles negative numbers and exact zeros implicitly.</li> </ul> <pre><code>from sklearn.preprocessing import PowerTransformer\n\n# Defaults to Yeo-Johnson\npt = PowerTransformer()\ndf[\"gaussian_price\"] = pt.fit_transform(df[[\"price\"]])\nprint(f\"Algorithm applied Lambda: {pt.lambdas_[0]}\")\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/descriptive-stats/06-advanced-summaries/#3-rank-transformation-non-parametric","title":"3. Rank Transformation (Non-Parametric)","text":"<p>For situations absolutely dominated by savage outliers where nothing fixes the skew, you abandon the values completely and sort the list. The smallest number becomes <code>1</code>, the second becomes <code>2</code>, the billion outlier becomes <code>3</code>. This permanently flattens the distribution into a uniform rectangle, erasing distances but guaranteeing model convergence stability.</p> <pre><code>df[\"rank_price\"] = df[\"price\"].rank()\n</code></pre>"},{"location":"month-01/week-02-statistics-probability/distributions/","title":"DISTRIBUTIONS","text":""},{"location":"month-01/week-02-statistics-probability/hypothesis-testing/","title":"HYPOTHESIS TESTING","text":""},{"location":"month-01/week-02-statistics-probability/probability/","title":"PROBABILITY","text":""},{"location":"month-01/week-03-core-ml-algorithms/decision-trees/","title":"DECISION TREES","text":""},{"location":"month-01/week-03-core-ml-algorithms/linear-regression/","title":"LINEAR REGRESSION","text":""},{"location":"month-01/week-03-core-ml-algorithms/logistic-regression/","title":"LOGISTIC REGRESSION","text":""},{"location":"month-01/week-04-pytorch-scikit-mlflow/mlflow-tracking/","title":"MLFLOW TRACKING","text":""},{"location":"month-01/week-04-pytorch-scikit-mlflow/pytorch-basics/","title":"PYTORCH BASICS","text":""},{"location":"month-01/week-04-pytorch-scikit-mlflow/scikit-learn/","title":"SCIKIT LEARN","text":""},{"location":"month-02/","title":"Month 2","text":""},{"location":"month-02/week-05-deep-learning-cnns-rnns/cnns/","title":"CNNS","text":""},{"location":"month-02/week-05-deep-learning-cnns-rnns/rnns/","title":"RNNS","text":""},{"location":"month-02/week-05-deep-learning-cnns-rnns/transformers-intro/","title":"TRANSFORMERS INTRO","text":""},{"location":"month-02/week-06-nlp-bert-huggingface/bert-finetuning/","title":"BERT FINETUNING","text":""},{"location":"month-02/week-06-nlp-bert-huggingface/huggingface-ecosystem/","title":"HUGGINGFACE ECOSYSTEM","text":""},{"location":"month-02/week-06-nlp-bert-huggingface/tokenization/","title":"TOKENIZATION","text":""},{"location":"month-02/week-07-llms-rag-vectordbs/llm-internals/","title":"LLM INTERNALS","text":""},{"location":"month-02/week-07-llms-rag-vectordbs/rag-pipelines/","title":"RAG PIPELINES","text":""},{"location":"month-02/week-07-llms-rag-vectordbs/vector-databases/","title":"VECTOR DATABASES","text":""},{"location":"month-02/week-08-llm-finetuning-eval/evaluation-ragas/","title":"EVALUATION RAGAS","text":""},{"location":"month-02/week-08-llm-finetuning-eval/lora-qlora/","title":"LORA QLORA","text":""},{"location":"month-02/week-08-llm-finetuning-eval/unsloth/","title":"UNSLOTH","text":""},{"location":"month-03/","title":"Month 3","text":""},{"location":"month-03/week-09-fastapi-docker-rest/docker-containers/","title":"DOCKER CONTAINERS","text":""},{"location":"month-03/week-09-fastapi-docker-rest/fastapi/","title":"FASTAPI","text":""},{"location":"month-03/week-09-fastapi-docker-rest/rest-api-design/","title":"REST API DESIGN","text":""},{"location":"month-03/week-10-mlops-mlflow-dvc-cicd/dvc-data-versioning/","title":"DVC DATA VERSIONING","text":""},{"location":"month-03/week-10-mlops-mlflow-dvc-cicd/github-actions-cicd/","title":"GITHUB ACTIONS CICD","text":""},{"location":"month-03/week-10-mlops-mlflow-dvc-cicd/mlops-pipelines/","title":"MLOPS PIPELINES","text":""},{"location":"month-03/week-11-cloud-aws-sagemaker/aws-fundamentals/","title":"AWS FUNDAMENTALS","text":""},{"location":"month-03/week-11-cloud-aws-sagemaker/s3-ec2/","title":"S3 EC2","text":""},{"location":"month-03/week-11-cloud-aws-sagemaker/sagemaker/","title":"SAGEMAKER","text":""},{"location":"month-03/week-12-kubernetes-monitoring/grafana/","title":"GRAFANA","text":""},{"location":"month-03/week-12-kubernetes-monitoring/kubernetes/","title":"KUBERNETES","text":""},{"location":"month-03/week-12-kubernetes-monitoring/prometheus/","title":"PROMETHEUS","text":""},{"location":"month-03/week-13-aws-cert-prep/cloud-practitioner/","title":"CLOUD PRACTITIONER","text":""},{"location":"month-03/week-13-aws-cert-prep/solutions-architect/","title":"SOLUTIONS ARCHITECT","text":""},{"location":"month-04/","title":"Month 4","text":""},{"location":"month-04/week-14-langgraph-multi-agent/crewai/","title":"CREWAI","text":""},{"location":"month-04/week-14-langgraph-multi-agent/langchain/","title":"LANGCHAIN","text":""},{"location":"month-04/week-14-langgraph-multi-agent/langgraph/","title":"LANGGRAPH","text":""},{"location":"month-04/week-15-data-pipelines-airflow-dbt/airflow/","title":"AIRFLOW","text":""},{"location":"month-04/week-15-data-pipelines-airflow-dbt/bigquery/","title":"BIGQUERY","text":""},{"location":"month-04/week-15-data-pipelines-airflow-dbt/dbt-transformations/","title":"DBT TRANSFORMATIONS","text":""},{"location":"month-04/week-16-spark-kafka-streaming/apache-spark/","title":"APACHE SPARK","text":""},{"location":"month-04/week-16-spark-kafka-streaming/kafka/","title":"KAFKA","text":""},{"location":"month-04/week-16-spark-kafka-streaming/streaming-basics/","title":"STREAMING BASICS","text":""},{"location":"month-04/week-17-system-design-capstone/capstone-planning/","title":"CAPSTONE PLANNING","text":""},{"location":"month-04/week-17-system-design-capstone/system-design/","title":"SYSTEM DESIGN","text":""},{"location":"month-05/","title":"Month 5","text":""},{"location":"month-05/week-18-ml-interview-theory/core-ml-questions/","title":"CORE ML QUESTIONS","text":""},{"location":"month-05/week-18-ml-interview-theory/theory-deep-dive/","title":"THEORY DEEP DIVE","text":""},{"location":"month-05/week-19-system-design-ml/llm-search-scale/","title":"LLM SEARCH SCALE","text":""},{"location":"month-05/week-19-system-design-ml/real-time-fraud/","title":"REAL TIME FRAUD","text":""},{"location":"month-05/week-20-mock-interviews/behavioral-prep/","title":"BEHAVIORAL PREP","text":""},{"location":"month-05/week-20-mock-interviews/coding-interviews/","title":"CODING INTERVIEWS","text":""},{"location":"month-05/week-21-kaggle-sprint-aws/aws-saa-exam/","title":"AWS SAA EXAM","text":""},{"location":"month-05/week-21-kaggle-sprint-aws/kaggle-competition/","title":"KAGGLE COMPETITION","text":""},{"location":"month-06/","title":"Month 6","text":""},{"location":"month-06/week-22-capstone-core-build/backend-development/","title":"BACKEND DEVELOPMENT","text":""},{"location":"month-06/week-22-capstone-core-build/model-deployment/","title":"MODEL DEPLOYMENT","text":""},{"location":"month-06/week-23-capstone-frontend-demo/live-demo-deployment/","title":"LIVE DEMO DEPLOYMENT","text":""},{"location":"month-06/week-23-capstone-frontend-demo/streamlit-gradio/","title":"STREAMLIT GRADIO","text":""},{"location":"month-06/week-24-tech-blog-arch-walkthrough/architecture-diagrams/","title":"ARCHITECTURE DIAGRAMS","text":""},{"location":"month-06/week-24-tech-blog-arch-walkthrough/blog-drafting/","title":"BLOG DRAFTING","text":""},{"location":"month-06/week-25-portfolio-polish/github-readme/","title":"GITHUB README","text":""},{"location":"month-06/week-25-portfolio-polish/resume-optimization/","title":"RESUME OPTIMIZATION","text":""},{"location":"month-06/week-26-career-launch/gcp-ace-prep/","title":"GCP ACE PREP","text":""},{"location":"month-06/week-26-career-launch/job-applications/","title":"JOB APPLICATIONS","text":""},{"location":"projects/","title":"Projects","text":""},{"location":"resources/","title":"Resources","text":""}]}